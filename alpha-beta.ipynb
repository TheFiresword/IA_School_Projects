{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descent-gradient algorithm implementation following:\n",
    "    Batch method\n",
    "    On-line method\n",
    "### And using different loss functions:<br>\n",
    "    L2 error\n",
    "    Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changment(loss_function, weights, label, features):\n",
    "    if loss_function == \"quadratic\":\n",
    "        factor = -2*(label - weights.T.dot(features))\n",
    "        return factor*features \n",
    "    elif loss_function == \"l1\":\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_changment(loss_function, weights, all_labels, all_features):\n",
    "    if loss_function in {\"quadratic\", \"MSE\"}:\n",
    "        delta = np.zeros(weights.shape)\n",
    "        for i in range(len(all_labels)):\n",
    "            delta += changment(\"quadratic\", weights, all_labels[i], all_features[i])\n",
    "        #print(f\"Delta is {delta}\")\n",
    "    return delta if loss_function == \"quadratic\" else delta/len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def descent_gradient(loss_function:str, labels:np.array, record_features:np.array, nepochs:int, learning_rate:float, method=\"batch\" or \"on-line\"):\n",
    "    size = len(labels)\n",
    "\n",
    "    init_shape = record_features.shape\n",
    "\n",
    "    nweights = 1 if len(init_shape) == 1 else init_shape[1]\n",
    "    weights = np.random.rand(nweights+1)\n",
    "\n",
    "    # Adding the feature 1 for the bias node\n",
    "    bias_feature = np.ones(size)\n",
    "\n",
    "    cpy_record_features = record_features.reshape((size, 1)) if nweights == 1 else record_features\n",
    "    cpy_record_features = np.hstack((cpy_record_features, bias_feature.reshape((size, 1))))\n",
    "    #print(cpy_record_features, cpy_record_features[0])\n",
    "\n",
    "    if method == \"on-line\":\n",
    "        for _ in range(nepochs):\n",
    "            for i in range(size):\n",
    "                weights = weights - learning_rate*changment(loss_function, weights, labels[i], cpy_record_features[i])\n",
    "    \n",
    "    elif method == \"batch\":\n",
    "        for _ in range(nepochs):\n",
    "            weights = weights - learning_rate*sum_changment(loss_function, weights, labels, cpy_record_features)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = \"quadratic\"\n",
    "nepochs = 10000\n",
    "learning_rate = 0.01\n",
    "labels = np.array([3.5, 5.2, 4.5, 5.3, 6.5, 4.1])\n",
    "features_X= np.array([0.1, 0.2, 0.3, 0.3, 0.5, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = descent_gradient(loss_function, labels, features_X, nepochs, learning_rate, \"batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(features_X, labels, 'ro', label=\"Original data\")\n",
    "z = np.linspace(0, 0.6, 100)\n",
    "plt.plot(z, w[0]*z+w[-1], 'b', label=\"Original data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multdm_features = np.array([\n",
    "    [0.5, 3],\n",
    "    [0.4, 3],\n",
    "    [0.4, 4],\n",
    "    [2.3, 5],\n",
    "    [2.1, 5],\n",
    "    [2.2, 4.5]\n",
    "])\n",
    "\n",
    "_w_ = descent_gradient(\"quadratic\", labels, multdm_features, 10000, 0.001, \"batch\")\n",
    "print(_w_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(multdm_features[:, 0], multdm_features[:, 1], 'ro', label=\"Original data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1, 1,  1, -1, -1, -1])\n",
    "\n",
    "def perceptron(cpy_multdm_features, labels, nepochs, learning_rate):\n",
    "\n",
    "    init_shape = cpy_multdm_features.shape\n",
    "    nweights = 1 if len(init_shape) == 1 else init_shape[1] -1\n",
    "    weights = np.random.rand(nweights+1)\n",
    "    \n",
    "\n",
    "    for _ in range(nepochs):\n",
    "        misclassified = 0\n",
    "        delta = 0\n",
    "        for i in range(len(labels)):\n",
    "            cost = labels[i]*weights.T.dot(cpy_multdm_features[i])\n",
    "            if cost <= 0:\n",
    "                misclassified += 1\n",
    "                delta += labels[i]*cpy_multdm_features[i]\n",
    "        weights = weights + learning_rate*delta\n",
    "    return weights\n",
    "\n",
    "cpy_multdm_features = np.hstack((multdm_features, np.ones(len(labels)).reshape((-1, 1))))\n",
    "final = perceptron(multdm_features, labels, 100, 0.1)\n",
    "#print(final)\n",
    "posCategorie = np.array([(multdm_features[:, 0][i], multdm_features[:, 1][i]) for i in range(len(labels)) if labels[i] == 1])\n",
    "negCategorie = np.array([(multdm_features[:, 0][i], multdm_features[:, 1][i]) for i in range(len(labels)) if labels[i] == -1])\n",
    "\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "\n",
    "z = np.linspace(0, 10, 10)\n",
    "separator_equation = (-final[0]*z-final[-1])/final[1]\n",
    "plt.plot(z, separator_equation, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris_data = pd.read_csv(\"C:/Users/Junior/Documents/3A/AI/TDs/iris_a.txt\", header=None)\n",
    "feature_array = np.array(iris_data)\n",
    "size = len(feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posCategorie = np.array([(feature_array[:, 0][i], feature_array[:, 1][i]) for i in range(size) if feature_array[:, -1][i] == 1])\n",
    "negCategorie = np.array([(feature_array[:, 0][i], feature_array[:, 1][i]) for i in range(size) if feature_array[:, -1][i] == -1])\n",
    "e = plt.figure(0)\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "e.show()\n",
    "\n",
    "\n",
    "posCategorie = np.array([(feature_array[:, 0][i], feature_array[:, 2][i]) for i in range(size) if feature_array[:, -1][i] == 1])\n",
    "negCategorie = np.array([(feature_array[:, 0][i], feature_array[:, 2][i]) for i in range(size) if feature_array[:, -1][i] == -1])\n",
    "\n",
    "f = plt.figure(1)\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "f.show()\n",
    "\n",
    "g = plt.figure(2)\n",
    "posCategorie = np.array([(feature_array[:, 0][i], feature_array[:, 3][i]) for i in range(size) if feature_array[:, -1][i] == 1])\n",
    "negCategorie = np.array([(feature_array[:, 0][i], feature_array[:, 3][i]) for i in range(size) if feature_array[:, -1][i] == -1])\n",
    "\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "g.show()\n",
    "\n",
    "h = plt.figure(3)\n",
    "posCategorie = np.array([(feature_array[:, 1][i], feature_array[:, 2][i]) for i in range(size) if feature_array[:, -1][i] == 1])\n",
    "negCategorie = np.array([(feature_array[:, 1][i], feature_array[:, 2][i]) for i in range(size) if feature_array[:, -1][i] == -1])\n",
    "\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "h.show()\n",
    "\n",
    "i = plt.figure(4)\n",
    "posCategorie = np.array([(feature_array[:, 1][i], feature_array[:, 3][i]) for i in range(size) if feature_array[:, -1][i] == 1])\n",
    "negCategorie = np.array([(feature_array[:, 1][i], feature_array[:, 3][i]) for i in range(size) if feature_array[:, -1][i] == -1])\n",
    "\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "i.show()\n",
    "\n",
    "j = plt.figure(5)\n",
    "posCategorie = np.array([(feature_array[:, 1][i], feature_array[:, 3][i]) for i in range(size) if feature_array[:, -1][i] == 1])\n",
    "negCategorie = np.array([(feature_array[:, 1][i], feature_array[:, 3][i]) for i in range(size) if feature_array[:, -1][i] == -1])\n",
    "\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "j.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of a Perceptron model on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(feature_array[:, 0:2], feature_array[:, -1])\n",
    "\n",
    "np.random.shuffle(feature_array)\n",
    "#print(feature_array)\n",
    "labels = feature_array[:, -1]\n",
    "cpy_multdm_features = np.hstack((feature_array[:, 0:2], np.ones(len(labels)).reshape((-1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "#------------------------------------------------------------#\n",
    "Splitting data in train set and test set\n",
    "\"\"\"\n",
    "#print(cpy_multdm_features[0:5,:])\n",
    "\n",
    "fifty = len(labels)//2\n",
    "train_set = cpy_multdm_features[0:fifty, :]\n",
    "train_y = labels[0:fifty]\n",
    "\n",
    "test_set = cpy_multdm_features[fifty: , :]\n",
    "test_y = labels[fifty:]\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "#Training the perceptron\n",
    "\n",
    "iris_weights = perceptron(train_set, train_y, nepochs, learning_rate)\n",
    "\n",
    "print(iris_weights)\n",
    "posCategorie = np.array([(feature_array[0:fifty, 0][i], feature_array[0:fifty, 1][i]) for i in range(len(train_y)) if feature_array[0:fifty, -1][i] == 1])\n",
    "negCategorie = np.array([(feature_array[0:fifty, 0][i], feature_array[0:fifty, 1][i]) for i in range(len(train_y)) if feature_array[0:fifty, -1][i] == -1])\n",
    "\n",
    "#print(posCategorie)\n",
    "#print(negCategorie)\n",
    "plt.plot(posCategorie[:, 0], posCategorie[:, 1], '+')\n",
    "plt.plot(negCategorie[:, 0], negCategorie[:, 1], 'o')\n",
    "\n",
    "z = np.linspace(0, 10, 10)\n",
    "separator_equation = (-iris_weights[0]*z-iris_weights[-1])/iris_weights[1]\n",
    "plt.plot(z, separator_equation, 'b')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[34mTP: 22 -- TN: 27 -- FP: 0 -- FN: 1 \u001b[0m\n",
      " \n",
      "[-10.665376629013952, 2.5007215068260873, -4.542451766392654, 1.980453861623703, -9.624841338609183, -7.770654141670358, -1.5548231507296018, -2.6217636919928786, 1.8337617309578695, 3.7280445544991405, 4.248312199701521, -5.809870440668144, 2.754010141555316, 1.0866107018847695, -4.62264301959754, -7.8508453948752415, 0.7531308139506587, 0.41965092601654796, -4.42216488658533, -3.688704233256159, -3.2887234678611, -4.729239523660928, -4.875931654326765, -2.3547846815197193, 3.768140181101577, -4.582547392995096, 2.16724161889198, 2.7276048906968082, -5.249507168863314, -3.688704233256159, 3.167681282694309, -3.808991113063483, -5.4763905527340295, -1.8482074120612688, 4.248312199701521, 1.4601862164213186, 2.460625880223647, -1.5010371483832274, 1.0866107018847695, 5.369038743311178, 2.2738381229553752, -5.582987056797425, 3.8346410585625286, 5.4756352473745675, -4.435855262329261, -5.329698422068198, 1.273398459153045, -3.808991113063483, -5.396199299529149, -6.916906608533864]\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------#\n",
    "#Testing the perceptron\n",
    "\n",
    "def predict(weights:np.array, test_set, test_y, metric=\"accuracy\"):\n",
    "    size = len(test_y)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(size):\n",
    "        prediction = weights.dot(test_set[i, :])\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return compute_metric(predictions, test_y, \"LogisticRegression\", metric)\n",
    "\n",
    "\n",
    "def compute_metric(predictions, labels, usage=\"LogisticRegression\", metric=\"accuracy\", error_function=\"percept\"):\n",
    "    #---------------------------------------------------------------\n",
    "    # Colors code to display metrics\n",
    "    #---------------------------------------------------------------\n",
    "    red = \"\\n\\033[31m\"\n",
    "    green = \"\\n\\033[32m\"\n",
    "    yellow = \"\\n\\033[33m\"\n",
    "    blue = \"\\n\\033[34m\"\n",
    "    magenta = \"\\n\\033[35m\"\n",
    "    cyan = \"\\n\\033[36m\"\n",
    "    reset = \"\\033[0m\\n\"\n",
    "\n",
    "    if usage in ['LogisticRegression', 'MultiClassification']:\n",
    "        size = len(labels)\n",
    "        \n",
    "        good_prediction= lambda prediction, label, error_function: prediction*label >=0 if error_function == \"percept\" else prediction==label if error_function == \"simple\" else 5\n",
    "        match metric:\n",
    "            case \"accuracy\" | \"acc\":\n",
    "                accuracy = 100*sum(good_prediction(predictions[i],labels[i],error_function) for i in range(size)) / size\n",
    "                print(f\" {red if accuracy < 50 else green}Your model accuracy is  {accuracy}% on this dataset {reset} \")\n",
    "                return accuracy, predictions\n",
    "                \n",
    "\n",
    "            case \"confusion_matrix\" | \"conf-mat\" | \"recall\" | \"precision\" | \"f1\"| \"au_roc\":\n",
    "                true_negatives, true_positives, false_negatives, false_positives = 0,0,0,0\n",
    "            \n",
    "                true_negatives = sum(predictions[i] < 0 and  good_prediction(predictions[i], test_y[i],error_function) for i in range(size))\n",
    "                true_positives = sum(predictions[i] > 0 and  good_prediction(predictions[i], test_y[i],error_function) for i in range(size))\n",
    "                false_negatives = sum(predictions[i] < 0 and not good_prediction(predictions[i], test_y[i],error_function) for i in range(size))\n",
    "                false_positives = sum(predictions[i] > 0 and not good_prediction(predictions[i], test_y[i],error_function) for i in range(size))\n",
    "\n",
    "                if metric in [\"confusion_matrix\",\"conf-mat\"]:\n",
    "                    print(f\" {blue}TP: {true_positives} -- TN: {true_negatives} -- FP: {false_positives} -- FN: {false_negatives} {reset} \")\n",
    "                    return np.array([true_positives, true_negatives, false_positives, false_negatives]), predictions\n",
    "                \n",
    "                if metric in [\"recall\",  \"f1\"]:\n",
    "                    recall = true_positives/(true_positives+false_positives)\n",
    "                    if metric == \"recall\":\n",
    "                        print(f\"{red if recall < 0.50 else green} Recall: {recall} {reset}\")\n",
    "                        return recall, predictions\n",
    "                \n",
    "                if metric in [\"precision\", \"f1\"]:\n",
    "                    precision = true_positives/(true_positives+false_negatives)\n",
    "                    if metric == \"precision\":\n",
    "                        print(f\" {red if precision < 50 else green} Precision: {precision} {reset}\")\n",
    "                        return precision, predictions\n",
    "                \n",
    "                if metric == \"f1\":\n",
    "                    f1_score = 2 / ((1/precision)+(1/recall))\n",
    "                    print(f\"{blue} f1 score: {f1_score} {reset} \")\n",
    "                    return f1_score, predictions\n",
    "                \n",
    "                if metric == \"au_roc\":\n",
    "                    tpr = true_positives/(true_positives+false_negatives)\n",
    "                    fpr =  false_positives/(false_positives+true_negatives)\n",
    "                    au_roc = tpr/fpr\n",
    "                    print(f\"{cyan} auc surface: {au_roc} {reset}\")\n",
    "                    return au_roc, predictions\n",
    "\n",
    "    else:\n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        match metric:\n",
    "            case \"MSE\":\n",
    "                mse = np.mean((predictions-labels))**2\n",
    "                print(f\"{cyan} Mean Square Error is: {mse}{reset} \")\n",
    "                return mse\n",
    "            case \"MSA\":\n",
    "                msa = np.mean((predictions-labels))\n",
    "                print(f\"{cyan}Mean Average Error is {msa}{reset}\")\n",
    "                return msa\n",
    "\n",
    "\n",
    "a,b =predict(iris_weights, test_set, test_y, \"conf-mat\")\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class Dense():\n",
    "    # -------------------------------------------------------------------#\n",
    "    # Fully connected layer\n",
    "    # -------------------------------------------------------------------#\n",
    "    def __init__(self, nodes_count:int, activation_function = \"reLu\", is_output=False, categories=[]) -> None:\n",
    "        # For the moment input should be like (x, )\n",
    "        # a node is an array that contains 5 values: \n",
    "        # an id --\n",
    "        # x::input data-- \n",
    "        # dE/dx:: derivative of error per x -- \n",
    "        # y:: output data = result of activation function f(x) -- \n",
    "        # dE/dy:: derivative of error per y\n",
    "        # input_nodes:: the nodes of the previous layer that are connected to this node\n",
    "        # CONVENTION the last node of a layer is the bias node\n",
    "        self.activation = activation_function\n",
    "        self.size = nodes_count +1 if not is_output else nodes_count\n",
    "        self.is_output = is_output\n",
    "        categories = categories[:nodes_count] if len(categories) >= nodes_count else categories\n",
    "        self.nodes = [\n",
    "            {'id':0, \n",
    "            'label':f'{categories[i]}' if i < len(categories) else '', \n",
    "            'x':0.0, \n",
    "            'dE/dx':0.0, \n",
    "            'y':0.0, \n",
    "            'dE/dy':0.0, \n",
    "            'input_nodes':[], \n",
    "            'output_nodes':[]\n",
    "            } \n",
    "        for i in range(self.size)]\n",
    "        \n",
    "    def activate(self):\n",
    "        match self.activation:\n",
    "            case \"identity\":\n",
    "                return lambda x: x\n",
    "            case \"reLu\": \n",
    "                return lambda x: x if x > 0 else 0\n",
    "            case \"sigmoid\":\n",
    "                return lambda x: math.exp(x) / (1 + math.exp(x))\n",
    "            \n",
    "    def derivate(self):\n",
    "         match self.activation:\n",
    "            case \"identity\":\n",
    "                return lambda _: 1\n",
    "            case \"reLu\":\n",
    "                 return lambda x: 0 if x <= 0 else 1\n",
    "            case \"sigmoid\":\n",
    "                return lambda x: (self.activate()(x)) * (1-(self.activate()(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetModel:\n",
    "    def __init__(self, input_shape, usage=\"LinearRegression\") -> None:\n",
    "        #-------------------------------------------------------------------\n",
    "        # For the moment, I manage 1D input data\n",
    "        #-------------------------------------------------------------------\n",
    "        self.input_shape = input_shape\n",
    "        self.layers_stack = []\n",
    "        self.weights = []\n",
    "        self.type = usage if usage in ['LinearRegression', 'LogisticRegression', 'MultiClassification'] else 'LinearRegression'\n",
    "        self.total_nodes = 0\n",
    "        self.losses = []\n",
    "        self.logistic_real_classes = []\n",
    "        # Add input layer\n",
    "        self.add_layer(Dense(self.input_shape[0], \"identity\"))\n",
    "        \n",
    "    \n",
    "    def add_layer(self,layer: Dense):\n",
    "        self.layers_stack.append(layer)\n",
    "        \n",
    "    \n",
    "    def compile(self, n_nodes=1, activation_function=\"reLu\", categories=[]):\n",
    "        \"\"\"\n",
    "        n_nodes:: Output layer nodes number; >1 for multiclassification\n",
    "\n",
    "        \"\"\"\n",
    "        if self.type == 'LogisticRegression':\n",
    "            activation_function = \"sigmoid\"\n",
    "        #-------------------------------------------------------------------\n",
    "        # First, add the output layer\n",
    "        #-------------------------------------------------------------------\n",
    "        if self.type == 'MultiClassification':\n",
    "            assert len(categories) >= 2 \n",
    "            self.add_layer(Dense(n_nodes, activation_function, is_output=True, categories=categories))\n",
    "        else:\n",
    "            self.add_layer(Dense(1, activation_function, is_output=True))\n",
    "\n",
    "\n",
    "        layers_count = len(self.layers_stack)\n",
    "        self.total_nodes = sum(self.layers_stack[i].size for i in range(layers_count)) \n",
    "        \n",
    "        #-------------------------------------------------------------------\n",
    "        # set nodes id\n",
    "        #-------------------------------------------------------------------\n",
    "        count_nodes = 0\n",
    "        for i in range(layers_count):\n",
    "            layer = self.layers_stack[i]\n",
    "            for k in range(layer.size):\n",
    "                layer.nodes[k]['id'] = count_nodes + k\n",
    "            count_nodes += layer.size\n",
    "\n",
    "        #-------------------------------------------------------------------\n",
    "        # add input_nodes\n",
    "        # Pay attention not to add input_nodes to bias node\n",
    "        # bias node is considered the last node of a layer\n",
    "        #-------------------------------------------------------------------\n",
    "        for i in range(1, layers_count):\n",
    "            layer = self.layers_stack[i]\n",
    "            back_layer = self.layers_stack[i-1]\n",
    "            back_nodes = [back_layer.nodes[a] for a in range(back_layer.size)]\n",
    "            right_range = range(layer.size -1) if i < layers_count-1 else range(layer.size)\n",
    "            for k in right_range:\n",
    "                layer.nodes[k]['input_nodes'] = back_nodes\n",
    "\n",
    "        #-------------------------------------------------------------------\n",
    "        # create weights and Add forward nodes\n",
    "        #-------------------------------------------------------------------\n",
    "        i = 0\n",
    "        while(i < layers_count - 1):\n",
    "            layer = self.layers_stack[i]\n",
    "            forward_layer = self.layers_stack[i+1]\n",
    "            right_range = range(forward_layer.size-1)if i < layers_count-2 else range(forward_layer.size)\n",
    "            forward_nodes = [forward_layer.nodes[i] for i in right_range ]\n",
    "\n",
    "            i += 1\n",
    "            for k in range(layer.size):\n",
    "                layer.nodes[k]['output_nodes'] = forward_nodes\n",
    "                for j in right_range:\n",
    "                    new_weight = {f\"w{layer.nodes[k]['id']}{forward_layer.nodes[j]['id']}\": random.random()}\n",
    "                    self.weights.append(new_weight) \n",
    "                \n",
    "            \n",
    "        # reformat weights\n",
    "        self.weights = np.array(self.weights)\n",
    "\n",
    "\n",
    "    def train(self,train_X, train_Y, cost_function=\"l2\", nepochs=100, learning_rate=0.01):\n",
    "        # -------------------------------------------------------------------#\n",
    "        # Transform labels into binary values if LogisticRegression Model\n",
    "        # -------------------------------------------------------------------#\n",
    "        if self.type == 'LogisticRegression':\n",
    "            train_Y = np.array(train_Y)\n",
    "            self.logistic_real_classes = np.unique(train_Y)\n",
    "            assert len(self.logistic_real_classes) == 2\n",
    "            train_Y = np.where((train_Y == min(self.logistic_real_classes)), 0, 1)\n",
    "\n",
    "        for _ in range(nepochs):\n",
    "            # -------------------------------------------------------------------#\n",
    "            # Iterate over the samples\n",
    "            # -------------------------------------------------------------------#\n",
    "            loss_history = np.array([])\n",
    "            for n in range(len(train_X)):\n",
    "                # -------------------------------------------------------------------#\n",
    "                # compute output\n",
    "                # -------------------------------------------------------------------#\n",
    "                self.forward_propagation(train_X[n])\n",
    "        \n",
    "                # -------------------------------------------------------------------#\n",
    "                # Compute error\n",
    "                # -------------------------------------------------------------------#\n",
    "                if self.type != \"MultiClassification\":\n",
    "                    output_node = self.layers_stack[-1].nodes[0]\n",
    "                    output = output_node['y']\n",
    "                    E, output_node['dE/dy']= self.compute_error(output, train_Y[n], cost_function)\n",
    "                    loss_history = np.append(loss_history, E)\n",
    "                else:\n",
    "                    pass\n",
    "                # -------------------------------------------------------------------#\n",
    "                # Backpropagate\n",
    "                # -------------------------------------------------------------------#\n",
    "                self.backpropagate(learning_rate)\n",
    "\n",
    "                \n",
    "            print(f\"Epoch {_}:  Loss is {np.mean(loss_history)} \")\n",
    "            self.losses.append(np.mean(loss_history))\n",
    "        return self.weights\n",
    "    \n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        input_layer = self.layers_stack[0]\n",
    "        # -------------------------------------------------------------------#\n",
    "        # Insert data into the input layer\n",
    "        # -------------------------------------------------------------------#\n",
    "        for i in range(input_layer.size):\n",
    "            # suppose ones column has been added for bias \n",
    "            input_layer.nodes[i]['x'] = input_data[i]\n",
    "            input_layer.nodes[i]['y'] = input_layer.activate()(input_layer.nodes[i]['x'])\n",
    "            \n",
    "        # -------------------------------------------------------------------#\n",
    "        # forward propagation\n",
    "        # -------------------------------------------------------------------#\n",
    "        for i in range(1, len(self.layers_stack)):\n",
    "            layer = self.layers_stack[i]\n",
    "            for j in range(layer.size):\n",
    "                # retrieve all nodes linked to this one in the previous layer\n",
    "                if layer.nodes[j]['input_nodes']:\n",
    "                    involved_weights = [self.get_weight_value(self.get_weight(str(f\"w{n_node['id']}{layer.nodes[j]['id']}\"))) \n",
    "                                        for n_node in layer.nodes[j]['input_nodes']]\n",
    "                    x_i = [n_node['y'] for n_node in layer.nodes[j]['input_nodes']]\n",
    "                    \n",
    "                    x_i = np.array(x_i).astype('float64')\n",
    "                    involved_weights = np.array(involved_weights).astype('float64')\n",
    "                    layer.nodes[j]['x'] = involved_weights.T.dot(x_i)\n",
    "                    layer.nodes[j]['y'] = layer.activate()(layer.nodes[j]['x'])\n",
    "        \n",
    "        output_layer = self.layers_stack[-1]\n",
    "        if self.type != 'MultiClassification':\n",
    "            return output_layer.nodes[0]['y']\n",
    "        else:\n",
    "            return [output_layer.nodes[i]['y'] for i in range(output_layer.size)]\n",
    "    \n",
    "\n",
    "    def compute_error(self, y_predicted, y_recorded, cost_function=\"l2\"):\n",
    "        match cost_function:\n",
    "            case \"l2\":\n",
    "                return (1/2)*((y_predicted-y_recorded)**2), (y_predicted-y_recorded)\n",
    "            case \"l1\":\n",
    "                return abs((y_predicted-y_recorded)), 1\n",
    "            case \"cross_entropy\":\n",
    "                epsilon = 1e-10\n",
    "                return -y_recorded*math.log(max(min(y_predicted, 1-epsilon), epsilon)) - (1-y_recorded)*math.log(max(min(1-y_predicted, 1-epsilon), epsilon)), (y_predicted-y_recorded)/(y_predicted*(1-y_predicted))\n",
    "\n",
    "\n",
    "    def backpropagate(self, learning_rate):\n",
    "        # -------------------------------------------------------------------#\n",
    "        # Backpropagate the prediction error into all hidden layers\n",
    "        # -------------------------------------------------------------------#\n",
    "        for i in range(len(self.layers_stack)-1, -1, -1):\n",
    "            layer =  self.layers_stack[i]\n",
    "            p_layer = self.layers_stack[i-1] if i-1 >= 0 else None\n",
    "            \n",
    "            # -------------------------------------------------------------------#\n",
    "            # For each node\n",
    "            # -------------------------------------------------------------------#\n",
    "            for layer_node in layer.nodes:\n",
    "                # -------------------------------------------------------------------#\n",
    "                # update dx\n",
    "                # -------------------------------------------------------------------#\n",
    "                layer_node['dE/dx'] = layer_node['dE/dy'] * layer.derivate()(layer_node['x'])\n",
    "                \n",
    "                # -------------------------------------------------------------------#\n",
    "                # update weights linked to this node\n",
    "                # -------------------------------------------------------------------#\n",
    "                if p_layer:\n",
    "                    for linked_node, id in [(l, str(l['id'])) for l in layer_node['input_nodes']]:\n",
    "                        w = self.get_weight(f\"w{id}{layer_node['id']}\")\n",
    "                        self.update_weight(w, -learning_rate*layer_node['dE/dx']*linked_node['y'])\n",
    "                        \n",
    "            # -------------------------------------------------------------------#\n",
    "            # update dy for layer before\n",
    "            # -------------------------------------------------------------------#\n",
    "            if p_layer:\n",
    "                for linked_node in p_layer.nodes:\n",
    "                    weight_i_j = np.array([self.get_weight_value(self.get_weight(f\"w{linked_node['id']}{node_['id']}\")) for node_ in linked_node['output_nodes']])\n",
    "                    derivatives_per_x_j = np.array([node_['dE/dx'] for node_ in linked_node['output_nodes']])\n",
    "                    linked_node['dE/dy'] = derivatives_per_x_j.T.dot(weight_i_j)\n",
    "\n",
    "    \n",
    "    def predict_unique(self, x:float, y:float=0.0, display_message=True):\n",
    "        prediction = self.forward_propagation(x)\n",
    "        match self.type:\n",
    "            case \"LinearRegression\":\n",
    "                if display_message: print(f\"Prediction = {prediction} -- Target = {y}\")\n",
    "                return prediction\n",
    "\n",
    "            case \"LogisticRegression\":\n",
    "                if prediction >= 0.5:\n",
    "                    r_prediction = self.logistic_real_classes[1]\n",
    "                else:\n",
    "                   r_prediction = self.logistic_real_classes[0]\n",
    "                \n",
    "                if display_message: print(f\"Prediction = {r_prediction} -- Target = {y}\") \n",
    "                return r_prediction\n",
    "\n",
    "\n",
    "    def predict_sample(self, X: list or np.array, Y :list or np.array, metric=\"\"):\n",
    "        assert len(X) == len(Y), \"The target data are not the same size as the samples\"\n",
    "        # -------------------------------------------------------------------#\n",
    "        # choose the appropriate metric if none is given\n",
    "        # -------------------------------------------------------------------#\n",
    "        if metric == \"\": metric = \"accuracy\" if self.type == \"LogisticRegression\" else \"MSE\" if self.type == \"LinerRegression\" else \"accuracy\"      \n",
    "        predictions = []\n",
    "        for record in X:\n",
    "            predictions.append(self.predict_unique(record, display_message=False))\n",
    "        compute_metric(predictions=predictions, labels=Y, usage=self.type, metric=metric, error_function=\"simple\")\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    def get_weight(self, label):\n",
    "        for w in self.weights:\n",
    "            if label in w:\n",
    "                return w\n",
    "\n",
    "\n",
    "    def get_weight_value(self, weight):\n",
    "        return next(iter(weight.values()))\n",
    "\n",
    "\n",
    "    def update_weight(self, weight, value):\n",
    "        weight[list(weight.keys())[0]] += value\n",
    "        pass\n",
    "\n",
    "                            \n",
    "    def describe(self):\n",
    "        # function to describe the layers and the nodes\n",
    "        i = 0\n",
    "        for layer in self.layers_stack:\n",
    "            print(f\"Layer {str(i)}\")\n",
    "            i += 1\n",
    "            tmp = []\n",
    "            for _node in layer.nodes:\n",
    "                tmp.append((_node['id'], _node['label'], len(_node['input_nodes']), len(_node['output_nodes'])))\n",
    "                \n",
    "            tmp = pd.DataFrame(tmp, columns=['id', 'label', 'inputs', 'outputs']).set_index('id')\n",
    "            print(tmp)\n",
    "            \n",
    "\n",
    "    def summarize(self):\n",
    "        print(f\"Input shape: {self.input_shape}\\n Weights: {self.weights}\\n Number of nodes: {self.total_nodes}\")\n",
    "    \n",
    "\n",
    "    def display_losses(self):\n",
    "        n = len(self.losses)\n",
    "        if n < 2:\n",
    "            self.losses.append(self.losses[0])\n",
    "            n += 1\n",
    "        plt.figure()\n",
    "        nepochs = range(1, n+1)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(nepochs, self.losses)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  Loss is 0.986281823142357 \n",
      "Epoch 1:  Loss is 0.14822795197724975 \n",
      "Epoch 2:  Loss is 0.09235483995304092 \n",
      "Epoch 3:  Loss is 0.0790865002224406 \n",
      "Epoch 4:  Loss is 0.06976562125135559 \n",
      "Epoch 5:  Loss is 0.06181217124290849 \n",
      "Epoch 6:  Loss is 0.05485306852347683 \n",
      "Epoch 7:  Loss is 0.048730932683734275 \n",
      "Epoch 8:  Loss is 0.04333336293991456 \n",
      "Epoch 9:  Loss is 0.03856763938441145 \n",
      "Epoch 10:  Loss is 0.034354486761763746 \n",
      "Epoch 11:  Loss is 0.030625458473792295 \n",
      "Epoch 12:  Loss is 0.027321244913619095 \n",
      "Epoch 13:  Loss is 0.024390345158852397 \n",
      "Epoch 14:  Loss is 0.021787960592232345 \n",
      "Epoch 15:  Loss is 0.019475058374962252 \n",
      "Epoch 16:  Loss is 0.017417574704068052 \n",
      "Epoch 17:  Loss is 0.01558573541625425 \n",
      "Epoch 18:  Loss is 0.013953475673224757 \n",
      "Epoch 19:  Loss is 0.012497943499467297 \n",
      "Epoch 20:  Loss is 0.011199074390346748 \n",
      "Epoch 21:  Loss is 0.010039226237314016 \n",
      "Epoch 22:  Loss is 0.009002865512396116 \n",
      "Epoch 23:  Loss is 0.008076297074099145 \n",
      "Epoch 24:  Loss is 0.007247431147458616 \n",
      "Epoch 25:  Loss is 0.006505582030074176 \n",
      "Epoch 26:  Loss is 0.0058412939150134366 \n",
      "Epoch 27:  Loss is 0.005246189926676146 \n",
      "Epoch 28:  Loss is 0.004712841058920439 \n",
      "Epoch 29:  Loss is 0.004234652204203497 \n",
      "Epoch 30:  Loss is 0.003805762883398518 \n",
      "Epoch 31:  Loss is 0.0034209606410157446 \n",
      "Epoch 32:  Loss is 0.003075605370401364 \n",
      "Epoch 33:  Loss is 0.0027655630869729696 \n",
      "Epoch 34:  Loss is 0.0024871478820943684 \n",
      "Epoch 35:  Loss is 0.002237070971996192 \n",
      "Epoch 36:  Loss is 0.002012395910400784 \n",
      "Epoch 37:  Loss is 0.0018104991645597308 \n",
      "Epoch 38:  Loss is 0.0016290353658988288 \n",
      "Epoch 39:  Loss is 0.0014659066414437446 \n",
      "Epoch 40:  Loss is 0.001319235513231394 \n",
      "Epoch 41:  Loss is 0.001187340922148179 \n",
      "Epoch 42:  Loss is 0.001068716991883781 \n",
      "Epoch 43:  Loss is 0.0009620141994702381 \n",
      "Epoch 44:  Loss is 0.0008660226624675869 \n",
      "Epoch 45:  Loss is 0.0007796572903394813 \n",
      "Epoch 46:  Loss is 0.0007019445798437933 \n",
      "Epoch 47:  Loss is 0.0006320108621112449 \n",
      "Epoch 48:  Loss is 0.0005690718331490733 \n",
      "Epoch 49:  Loss is 0.0005124232203353151 \n",
      "Epoch 50:  Loss is 0.00046143245552762756 \n",
      "Epoch 51:  Loss is 0.0004155312410931385 \n",
      "Epoch 52:  Loss is 0.0003742089088073464 \n",
      "Epoch 53:  Loss is 0.0003370064834558902 \n",
      "Epoch 54:  Loss is 0.00030351137334585365 \n",
      "Epoch 55:  Loss is 0.00027335261899935176 \n",
      "Epoch 56:  Loss is 0.0002461966392390309 \n",
      "Epoch 57:  Loss is 0.00022174342083368162 \n",
      "Epoch 58:  Loss is 0.00019972310398194888 \n",
      "Epoch 59:  Loss is 0.00017989292128434874 \n",
      "Epoch 60:  Loss is 0.0001620344525841458 \n",
      "Epoch 61:  Loss is 0.00014595116222806875 \n",
      "Epoch 62:  Loss is 0.00013146618897973605 \n",
      "Epoch 63:  Loss is 0.00011842036207288931 \n",
      "Epoch 64:  Loss is 0.00010667041977146446 \n",
      "Epoch 65:  Loss is 9.608740935471736e-05 \n",
      "Epoch 66:  Loss is 8.65552497086699e-05 \n",
      "Epoch 67:  Loss is 7.796943971367755e-05 \n",
      "Epoch 68:  Loss is 7.023589740333013e-05 \n",
      "Epoch 69:  Loss is 6.326991645771102e-05 \n",
      "Epoch 70:  Loss is 5.6995228007575364e-05 \n",
      "Epoch 71:  Loss is 5.1343156985513245e-05 \n",
      "Epoch 72:  Loss is 4.625186338320515e-05 \n",
      "Epoch 73:  Loss is 4.166565977590117e-05 \n",
      "Epoch 74:  Loss is 3.753439737005085e-05 \n",
      "Epoch 75:  Loss is 3.381291362949456e-05 \n",
      "Epoch 76:  Loss is 3.0460535250340398e-05 \n",
      "Epoch 77:  Loss is 2.744063089402981e-05 \n",
      "Epoch 78:  Loss is 2.472020866028876e-05 \n",
      "Epoch 79:  Loss is 2.226955379409054e-05 \n",
      "Epoch 80:  Loss is 2.0061902579687378e-05 \n",
      "Epoch 81:  Loss is 1.807314878624419e-05 \n",
      "Epoch 82:  Loss is 1.6281579398361387e-05 \n",
      "Epoch 83:  Loss is 1.4667636695599074e-05 \n",
      "Epoch 84:  Loss is 1.3213704041937966e-05 \n",
      "Epoch 85:  Loss is 1.1903913012449856e-05 \n",
      "Epoch 86:  Loss is 1.0723969723633006e-05 \n",
      "Epoch 87:  Loss is 9.660998448579564e-06 \n",
      "Epoch 88:  Loss is 8.703400791059308e-06 \n",
      "Epoch 89:  Loss is 7.840728865910514e-06 \n",
      "Epoch 90:  Loss is 7.0635710888429364e-06 \n",
      "Epoch 91:  Loss is 6.363449318762344e-06 \n",
      "Epoch 92:  Loss is 5.732726221544856e-06 \n",
      "Epoch 93:  Loss is 5.164521837337142e-06 \n",
      "Epoch 94:  Loss is 4.652638435220633e-06 \n",
      "Epoch 95:  Loss is 4.191492830565325e-06 \n",
      "Epoch 96:  Loss is 3.7760554227427166e-06 \n",
      "Epoch 97:  Loss is 3.4017952849069894e-06 \n",
      "Epoch 98:  Loss is 3.0646307041902362e-06 \n",
      "Epoch 99:  Loss is 2.7608846306183574e-06 \n",
      "Epoch 100:  Loss is 2.487244547010315e-06 \n",
      "Epoch 101:  Loss is 2.240726320679119e-06 \n",
      "Epoch 102:  Loss is 2.0186416414694136e-06 \n",
      "Epoch 103:  Loss is 1.8185686900126153e-06 \n",
      "Epoch 104:  Loss is 1.638325715494703e-06 \n",
      "Epoch 105:  Loss is 1.4759472341152273e-06 \n",
      "Epoch 106:  Loss is 1.329662588132962e-06 \n",
      "Epoch 107:  Loss is 1.1978766312355474e-06 \n",
      "Epoch 108:  Loss is 1.0791523292432312e-06 \n",
      "Epoch 109:  Loss is 9.721950861180725e-07 \n",
      "Epoch 110:  Loss is 8.758386241175914e-07 \n",
      "Epoch 111:  Loss is 7.89032263927876e-07 \n",
      "Epoch 112:  Loss is 7.108294659129859e-07 \n",
      "Epoch 113:  Loss is 6.403775074100011e-07 \n",
      "Epoch 114:  Loss is 5.769081833956265e-07 \n",
      "Epoch 115:  Loss is 5.197294290446742e-07 \n",
      "Epoch 116:  Loss is 4.682177727667849e-07 \n",
      "Epoch 117:  Loss is 4.2181153737162777e-07 \n",
      "Epoch 118:  Loss is 3.800047151839112e-07 \n",
      "Epoch 119:  Loss is 3.423414502923565e-07 \n",
      "Epoch 120:  Loss is 3.0841106773773806e-07 \n",
      "Epoch 121:  Loss is 2.778435954173452e-07 \n",
      "Epoch 122:  Loss is 2.5030572985996803e-07 \n",
      "Epoch 123:  Loss is 2.254972018697521e-07 \n",
      "Epoch 124:  Loss is 2.03147502399226e-07 \n",
      "Epoch 125:  Loss is 1.830129329445443e-07 \n",
      "Epoch 126:  Loss is 1.6487394829240567e-07 \n",
      "Epoch 127:  Loss is 1.485327626408973e-07 \n",
      "Epoch 128:  Loss is 1.338111929879806e-07 \n",
      "Epoch 129:  Loss is 1.2054871626910306e-07 \n",
      "Epoch 130:  Loss is 1.0860071905889081e-07 \n",
      "Epoch 131:  Loss is 9.783692074869732e-08 \n",
      "Epoch 132:  Loss is 8.813995300628256e-08 \n",
      "Epoch 133:  Loss is 7.940408002818298e-08 \n",
      "Epoch 134:  Loss is 7.153404562969104e-08 \n",
      "Epoch 135:  Loss is 6.444403460064939e-08 \n",
      "Epoch 136:  Loss is 5.8056737002877345e-08 \n",
      "Epoch 137:  Loss is 5.230250520551163e-08 \n",
      "Epoch 138:  Loss is 4.711859446743593e-08 \n",
      "Epoch 139:  Loss is 4.2448478785331035e-08 \n",
      "Epoch 140:  Loss is 3.824123454931805e-08 \n",
      "Epoch 141:  Loss is 3.4450985283919105e-08 \n",
      "Epoch 142:  Loss is 3.103640142134583e-08 \n",
      "Epoch 143:  Loss is 2.7960249652513502e-08 \n",
      "Epoch 144:  Loss is 2.5188986941736293e-08 \n",
      "Epoch 145:  Loss is 2.269239477936856e-08 \n",
      "Epoch 146:  Loss is 2.0443249683112097e-08 \n",
      "Epoch 147:  Loss is 1.8417026356597986e-08 \n",
      "Epoch 148:  Loss is 1.6591630267693975e-08 \n",
      "Epoch 149:  Loss is 1.494715673203413e-08 \n",
      "Epoch 150:  Loss is 1.3465673872816792e-08 \n",
      "Epoch 151:  Loss is 1.2131027092341017e-08 \n",
      "Epoch 152:  Loss is 1.092866292219856e-08 \n",
      "Epoch 153:  Loss is 9.845470332309495e-09 \n",
      "Epoch 154:  Loss is 8.869637767561784e-09 \n",
      "Epoch 155:  Loss is 7.990524353816036e-09 \n",
      "Epoch 156:  Loss is 7.1985438690288175e-09 \n",
      "Epoch 157:  Loss is 6.485060213905432e-09 \n",
      "Epoch 158:  Loss is 5.84229324284602e-09 \n",
      "Epoch 159:  Loss is 5.263233928118545e-09 \n",
      "Epoch 160:  Loss is 4.7415679321539215e-09 \n",
      "Epoch 161:  Loss is 4.27160675473474e-09 \n",
      "Epoch 162:  Loss is 3.848225704455595e-09 \n",
      "Epoch 163:  Loss is 3.466808017831395e-09 \n",
      "Epoch 164:  Loss is 3.1231945169872896e-09 \n",
      "Epoch 165:  Loss is 2.8136382567731034e-09 \n",
      "Epoch 166:  Loss is 2.534763667140671e-09 \n",
      "Epoch 167:  Loss is 2.2835297446898973e-09 \n",
      "Epoch 168:  Loss is 2.0571968927092803e-09 \n",
      "Epoch 169:  Loss is 1.853297047620599e-09 \n",
      "Epoch 170:  Loss is 1.66960676632029e-09 \n",
      "Epoch 171:  Loss is 1.504122980906327e-09 \n",
      "Epoch 172:  Loss is 1.3550411564994594e-09 \n",
      "Epoch 173:  Loss is 1.22073561379998e-09 \n",
      "Epoch 174:  Loss is 1.0997418019880365e-09 \n",
      "Epoch 175:  Loss is 9.907403286469855e-10 \n",
      "Epoch 176:  Loss is 8.925425725195376e-10 \n",
      "Epoch 177:  Loss is 8.040777222074164e-10 \n",
      "Epoch 178:  Loss is 7.24381099525706e-10 \n",
      "Epoch 179:  Loss is 6.525836401907756e-10 \n",
      "Epoch 180:  Loss is 5.879024171276954e-10 \n",
      "Epoch 181:  Loss is 5.296321029763881e-10 \n",
      "Epoch 182:  Loss is 4.771372788526756e-10 \n",
      "Epoch 183:  Loss is 4.2984550543299835e-10 \n",
      "Epoch 184:  Loss is 3.872410807311193e-10 \n",
      "Epoch 185:  Loss is 3.488594165717548e-10 \n",
      "Epoch 186:  Loss is 3.1428197249955624e-10 \n",
      "Epoch 187:  Loss is 2.8313169174813416e-10 \n",
      "Epoch 188:  Loss is 2.5506888960305675e-10 \n",
      "Epoch 189:  Loss is 2.2978754925741552e-10 \n",
      "Epoch 190:  Loss is 2.0701198484359255e-10 \n",
      "Epoch 191:  Loss is 1.8649383515971905e-10 \n",
      "Epoch 192:  Loss is 1.680093554037232e-10 \n",
      "Epoch 193:  Loss is 1.5135697730458149e-10 \n",
      "Epoch 194:  Loss is 1.363551111136339e-10 \n",
      "Epoch 195:  Loss is 1.2284016541179382e-10 \n",
      "Epoch 196:  Loss is 1.1066476321135576e-10 \n",
      "Epoch 197:  Loss is 9.969613484306184e-11 \n",
      "Epoch 198:  Loss is 8.981467017611083e-11 \n",
      "Epoch 199:  Loss is 8.091261429291867e-11 \n",
      "Epoch 200:  Loss is 7.289289245644463e-11 \n",
      "Epoch 201:  Loss is 6.56680515426967e-11 \n",
      "Epoch 202:  Loss is 5.915930638744629e-11 \n",
      "Epoch 203:  Loss is 5.329568066504413e-11 \n",
      "Epoch 204:  Loss is 4.801323290758925e-11 \n",
      "Epoch 205:  Loss is 4.325435924456756e-11 \n",
      "Epoch 206:  Loss is 3.896716524336171e-11 \n",
      "Epoch 207:  Loss is 3.51049000263019e-11 \n",
      "Epoch 208:  Loss is 3.1625446456470993e-11 \n",
      "Epoch 209:  Loss is 2.849086186346888e-11 \n",
      "Epoch 210:  Loss is 2.56669642932621e-11 \n",
      "Epoch 211:  Loss is 2.3122959754605733e-11 \n",
      "Epoch 212:  Loss is 2.083110643221464e-11 \n",
      "Epoch 213:  Loss is 1.8766412148410025e-11 \n",
      "Epoch 214:  Loss is 1.6906361852346493e-11 \n",
      "Epoch 215:  Loss is 1.5230672088169197e-11 \n",
      "Epoch 216:  Loss is 1.3721069808600623e-11 \n",
      "Epoch 217:  Loss is 1.236109311283509e-11 \n",
      "Epoch 218:  Loss is 1.113591173332586e-11 \n",
      "Epoch 219:  Loss is 1.0032165319955675e-11 \n",
      "Epoch 220:  Loss is 9.037817737646767e-12 \n",
      "Epoch 221:  Loss is 8.142025827753587e-12 \n",
      "Epoch 222:  Loss is 7.335021160570847e-12 \n",
      "Epoch 223:  Loss is 6.60800350756028e-12 \n",
      "Epoch 224:  Loss is 5.953044884354715e-12 \n",
      "Epoch 225:  Loss is 5.363003098684259e-12 \n",
      "Epoch 226:  Loss is 4.831443858557263e-12 \n",
      "Epoch 227:  Loss is 4.352570613768518e-12 \n",
      "Epoch 228:  Loss is 3.921161343772275e-12 \n",
      "Epoch 229:  Loss is 3.5325116135808153e-12 \n",
      "Epoch 230:  Loss is 3.1823832719008595e-12 \n",
      "Epoch 231:  Loss is 2.8669582352952037e-12 \n",
      "Epoch 232:  Loss is 2.582796854152102e-12 \n",
      "Epoch 233:  Loss is 2.3268004020766526e-12 \n",
      "Epoch 234:  Loss is 2.096177285248643e-12 \n",
      "Epoch 235:  Loss is 1.8884126023326654e-12 \n",
      "Epoch 236:  Loss is 1.7012407180354496e-12 \n",
      "Epoch 237:  Loss is 1.5326205569888479e-12 \n",
      "Epoch 238:  Loss is 1.3807133476114281e-12 \n",
      "Epoch 239:  Loss is 1.2438625699666599e-12 \n",
      "Epoch 240:  Loss is 1.1205758918409464e-12 \n",
      "Epoch 241:  Loss is 1.009508893615699e-12 \n",
      "Epoch 242:  Loss is 9.094504104736823e-13 \n",
      "Epoch 243:  Loss is 8.193093225538271e-13 \n",
      "Epoch 244:  Loss is 7.381026575613814e-13 \n",
      "Epoch 245:  Loss is 6.649448717987189e-13 \n",
      "Epoch 246:  Loss is 5.990381927235587e-13 \n",
      "Epoch 247:  Loss is 5.396639192047726e-13 \n",
      "Epoch 248:  Loss is 4.8617458583779e-13 \n",
      "Epoch 249:  Loss is 4.3798690119156923e-13 \n",
      "Epoch 250:  Loss is 3.945753871781862e-13 \n",
      "Epoch 251:  Loss is 3.554666488905668e-13 \n",
      "Epoch 252:  Loss is 3.202342123968538e-13 \n",
      "Epoch 253:  Loss is 2.8849387423000706e-13 \n",
      "Epoch 254:  Loss is 2.598995113500423e-13 \n",
      "Epoch 255:  Loss is 2.3413930778423117e-13 \n",
      "Epoch 256:  Loss is 2.1093235259743763e-13 \n",
      "Epoch 257:  Loss is 1.900255781189501e-13 \n",
      "Epoch 258:  Loss is 1.7119099962013906e-13 \n",
      "Epoch 259:  Loss is 1.5422322946714189e-13 \n",
      "Epoch 260:  Loss is 1.389372369644101e-13 \n",
      "Epoch 261:  Loss is 1.2516633112765236e-13 \n",
      "Epoch 262:  Loss is 1.127603424314001e-13 \n",
      "Epoch 263:  Loss is 1.0158398590170887e-13 \n",
      "Epoch 264:  Loss is 9.1515385334573e-14 \n",
      "Epoch 265:  Loss is 8.244474448465748e-14 \n",
      "Epoch 266:  Loss is 7.427314931733365e-14 \n",
      "Epoch 267:  Loss is 6.691149012324442e-14 \n",
      "Epoch 268:  Loss is 6.027948932771998e-14 \n",
      "Epoch 269:  Loss is 5.4304826015104164e-14 \n",
      "Epoch 270:  Loss is 4.8922347507015414e-14 \n",
      "Epoch 271:  Loss is 4.407335888724763e-14 \n",
      "Epoch 272:  Loss is 3.9704982711270165e-14 \n",
      "Epoch 273:  Loss is 3.5769582690650406e-14 \n",
      "Epoch 274:  Loss is 3.2224243786265326e-14 \n",
      "Epoch 275:  Loss is 2.903030476977799e-14 \n",
      "Epoch 276:  Loss is 2.6152936264062497e-14 \n",
      "Epoch 277:  Loss is 2.3560761101749035e-14 \n",
      "Epoch 278:  Loss is 2.1225512072665413e-14 \n",
      "Epoch 279:  Loss is 1.9121723619721356e-14 \n",
      "Epoch 280:  Loss is 1.722645427866527e-14 \n",
      "Epoch 281:  Loss is 1.551903651903178e-14 \n",
      "Epoch 282:  Loss is 1.3980851259490811e-14 \n",
      "Epoch 283:  Loss is 1.2595124794271625e-14 \n",
      "Epoch 284:  Loss is 1.1346746095615908e-14 \n",
      "Epoch 285:  Loss is 1.0222101651226557e-14 \n",
      "Epoch 286:  Loss is 9.208927532833573e-15 \n",
      "Epoch 287:  Loss is 8.296175233890729e-15 \n",
      "Epoch 288:  Loss is 7.473891258259598e-15 \n",
      "Epoch 289:  Loss is 6.733108803740158e-15 \n",
      "Epoch 290:  Loss is 6.065749801782604e-15 \n",
      "Epoch 291:  Loss is 5.464536762727486e-15 \n",
      "Epoch 292:  Loss is 4.9229135786102924e-15 \n",
      "Epoch 293:  Loss is 4.434973926726557e-15 \n",
      "Epoch 294:  Loss is 3.995396928219766e-15 \n",
      "Epoch 295:  Loss is 3.5993890383455075e-15 \n",
      "Epoch 296:  Loss is 3.242631871765995e-15 \n",
      "Epoch 297:  Loss is 2.92123507949639e-15 \n",
      "Epoch 298:  Loss is 2.6316938487084675e-15 \n",
      "Epoch 299:  Loss is 2.3708507863820466e-15 \n",
      "Epoch 300:  Loss is 2.1358614620565135e-15 \n",
      "Epoch 301:  Loss is 1.9241633242964547e-15 \n",
      "Epoch 302:  Loss is 1.7334478885820984e-15 \n",
      "Epoch 303:  Loss is 1.5616354058600046e-15 \n",
      "Epoch 304:  Loss is 1.4068522787129734e-15 \n",
      "Epoch 305:  Loss is 1.2674106586200581e-15 \n",
      "Epoch 306:  Loss is 1.1417899347823586e-15 \n",
      "Epoch 307:  Loss is 1.0286202388450568e-15 \n",
      "Epoch 308:  Loss is 9.266674857692344e-16 \n",
      "Epoch 309:  Loss is 8.348198731645628e-16 \n",
      "Epoch 310:  Loss is 7.520758421992685e-16 \n",
      "Epoch 311:  Loss is 6.775330779814909e-16 \n",
      "Epoch 312:  Loss is 6.103786873411992e-16 \n",
      "Epoch 313:  Loss is 5.498803717281696e-16 \n",
      "Epoch 314:  Loss is 4.953784183096924e-16 \n",
      "Epoch 315:  Loss is 4.462784795007155e-16 \n",
      "Epoch 316:  Loss is 4.020451284603938e-16 \n",
      "Epoch 317:  Loss is 3.62196007226314e-16 \n",
      "Epoch 318:  Loss is 3.2629657265983256e-16 \n",
      "Epoch 319:  Loss is 2.9395535827218823e-16 \n",
      "Epoch 320:  Loss is 2.648196640843868e-16 \n",
      "Epoch 321:  Loss is 2.3857178727068156e-16 \n",
      "Epoch 322:  Loss is 2.1492549175393719e-16 \n",
      "Epoch 323:  Loss is 1.9362292719920457e-16 \n",
      "Epoch 324:  Loss is 1.7443179016932799e-16 \n",
      "Epoch 325:  Loss is 1.5714280574077923e-16 \n",
      "Epoch 326:  Loss is 1.4156743699498068e-16 \n",
      "Epoch 327:  Loss is 1.275358341970421e-16 \n",
      "Epoch 328:  Loss is 1.1489498559917202e-16 \n",
      "Epoch 329:  Loss is 1.035070482615101e-16 \n",
      "Epoch 330:  Loss is 9.32478379577383e-17 \n",
      "Epoch 331:  Loss is 8.400548120636473e-17 \n",
      "Epoch 332:  Loss is 7.567919213026183e-17 \n",
      "Epoch 333:  Loss is 6.81781715116284e-17 \n",
      "Epoch 334:  Loss is 6.142062057870202e-17 \n",
      "Epoch 335:  Loss is 5.533285227392484e-17 \n",
      "Epoch 336:  Loss is 4.9848479716629196e-17 \n",
      "Epoch 337:  Loss is 4.4907692784187044e-17 \n",
      "Epoch 338:  Loss is 4.045661947171761e-17 \n",
      "Epoch 339:  Loss is 3.6446716875624795e-17 \n",
      "Epoch 340:  Loss is 3.283426296423718e-17 \n",
      "Epoch 341:  Loss is 2.957986223224497e-17 \n",
      "Epoch 342:  Loss is 2.6648025450317834e-17 \n",
      "Epoch 343:  Loss is 2.4006779863568255e-17 \n",
      "Epoch 344:  Loss is 2.1627324648973883e-17 \n",
      "Epoch 345:  Loss is 1.948370786800073e-17 \n",
      "Epoch 346:  Loss is 1.7552557666045534e-17 \n",
      "Epoch 347:  Loss is 1.5812817990612734e-17 \n",
      "Epoch 348:  Loss is 1.4245515259447505e-17 \n",
      "Epoch 349:  Loss is 1.2833557420538036e-17 \n",
      "Epoch 350:  Loss is 1.1561544455533848e-17 \n",
      "Epoch 351:  Loss is 1.0415610460710725e-17 \n",
      "Epoch 352:  Loss is 9.383256669883312e-18 \n",
      "Epoch 353:  Loss is 8.453225628147556e-18 \n",
      "Epoch 354:  Loss is 7.615375796822162e-18 \n",
      "Epoch 355:  Loss is 6.8605705797653685e-18 \n",
      "Epoch 356:  Loss is 6.180578058533921e-18 \n",
      "Epoch 357:  Loss is 5.567984708490196e-18 \n",
      "Epoch 358:  Loss is 5.016108210020952e-18 \n",
      "Epoch 359:  Loss is 4.518931220907992e-18 \n",
      "Epoch 360:  Loss is 4.0710322113074544e-18 \n",
      "Epoch 361:  Loss is 3.667527589856372e-18 \n",
      "Epoch 362:  Loss is 3.304016000355087e-18 \n",
      "Epoch 363:  Loss is 2.976534740456396e-18 \n",
      "Epoch 364:  Loss is 2.681511975867175e-18 \n",
      "Epoch 365:  Loss is 2.4157313031777784e-18 \n",
      "Epoch 366:  Loss is 2.1762938198688173e-18 \n",
      "Epoch 367:  Loss is 1.9605884153316858e-18 \n",
      "Epoch 368:  Loss is 1.766263205963419e-18 \n",
      "Epoch 369:  Loss is 1.5911985947622912e-18 \n",
      "Epoch 370:  Loss is 1.4334852841100183e-18 \n",
      "Epoch 371:  Loss is 1.2914042149583408e-18 \n",
      "Epoch 372:  Loss is 1.163405695777481e-18 \n",
      "Epoch 373:  Loss is 1.0480941064509568e-18 \n",
      "Epoch 374:  Loss is 9.442113166488372e-19 \n",
      "Epoch 375:  Loss is 8.506250007369732e-19 \n",
      "Epoch 376:  Loss is 7.6631460400937505e-19 \n",
      "Epoch 377:  Loss is 6.903603059552967e-19 \n",
      "Epoch 378:  Loss is 6.219343796373462e-19 \n",
      "Epoch 379:  Loss is 5.602905959237823e-19 \n",
      "Epoch 380:  Loss is 5.0475679597827785e-19 \n",
      "Epoch 381:  Loss is 4.547272321538113e-19 \n",
      "Epoch 382:  Loss is 4.096564474081728e-19 \n",
      "Epoch 383:  Loss is 3.6905307038534684e-19 \n",
      "Epoch 384:  Loss is 3.3247389997129904e-19 \n",
      "Epoch 385:  Loss is 2.995204294982471e-19 \n",
      "Epoch 386:  Loss is 2.698334108759401e-19 \n",
      "Epoch 387:  Loss is 2.4308848502425375e-19 \n",
      "Epoch 388:  Loss is 2.1899443115162975e-19 \n",
      "Epoch 389:  Loss is 1.9728859524648576e-19 \n",
      "Epoch 390:  Loss is 1.7773397620720217e-19 \n",
      "Epoch 391:  Loss is 1.6011762947175878e-19 \n",
      "Epoch 392:  Loss is 1.4424752976661958e-19 \n",
      "Epoch 393:  Loss is 1.2995028858186403e-19 \n",
      "Epoch 394:  Loss is 1.1707028325144946e-19 \n",
      "Epoch 395:  Loss is 1.0546681747031206e-19 \n",
      "Epoch 396:  Loss is 9.501324240108202e-20 \n",
      "Epoch 397:  Loss is 8.559591482640316e-20 \n",
      "Epoch 398:  Loss is 7.711208947188045e-20 \n",
      "Epoch 399:  Loss is 6.946902739913171e-20 \n",
      "Epoch 400:  Loss is 6.258355127833802e-20 \n",
      "Epoch 401:  Loss is 5.638052405499995e-20 \n",
      "Epoch 402:  Loss is 5.0792332491423415e-20 \n",
      "Epoch 403:  Loss is 4.5758017823614843e-20 \n",
      "Epoch 404:  Loss is 4.1222743199567775e-20 \n",
      "Epoch 405:  Loss is 3.7136937440733776e-20 \n",
      "Epoch 406:  Loss is 3.3456116840197296e-20 \n",
      "Epoch 407:  Loss is 3.014013988520073e-20 \n",
      "Epoch 408:  Loss is 2.715278225015419e-20 \n",
      "Epoch 409:  Loss is 2.4461472797627298e-20 \n",
      "Epoch 410:  Loss is 2.2036976104952454e-20 \n",
      "Epoch 411:  Loss is 1.9852723072319495e-20 \n",
      "Epoch 412:  Loss is 1.7885010200926266e-20 \n",
      "Epoch 413:  Loss is 1.6112346044630367e-20 \n",
      "Epoch 414:  Loss is 1.4515400977144725e-20 \n",
      "Epoch 415:  Loss is 1.3076677849094334e-20 \n",
      "Epoch 416:  Loss is 1.17805566817286e-20 \n",
      "Epoch 417:  Loss is 1.061293619740354e-20 \n",
      "Epoch 418:  Loss is 9.560981953229385e-21 \n",
      "Epoch 419:  Loss is 8.613357431479042e-21 \n",
      "Epoch 420:  Loss is 7.759649506391209e-21 \n",
      "Epoch 421:  Loss is 6.990546824545829e-21 \n",
      "Epoch 422:  Loss is 6.297671363432971e-21 \n",
      "Epoch 423:  Loss is 5.673463280431797e-21 \n",
      "Epoch 424:  Loss is 5.111124949104216e-21 \n",
      "Epoch 425:  Loss is 4.604553518622397e-21 \n",
      "Epoch 426:  Loss is 4.1481844077999585e-21 \n",
      "Epoch 427:  Loss is 3.737034969530621e-21 \n",
      "Epoch 428:  Loss is 3.36664761494123e-21 \n",
      "Epoch 429:  Loss is 3.0329596869207606e-21 \n",
      "Epoch 430:  Loss is 2.732333615137494e-21 \n",
      "Epoch 431:  Loss is 2.461497464642782e-21 \n",
      "Epoch 432:  Loss is 2.2175351765956986e-21 \n",
      "Epoch 433:  Loss is 1.9977407630089923e-21 \n",
      "Epoch 434:  Loss is 1.7997475410817518e-21 \n",
      "Epoch 435:  Loss is 1.6213568465851483e-21 \n",
      "Epoch 436:  Loss is 1.460658843556791e-21 \n",
      "Epoch 437:  Loss is 1.31586810539118e-21 \n",
      "Epoch 438:  Loss is 1.1854493551898356e-21 \n",
      "Epoch 439:  Loss is 1.0679606674051273e-21 \n",
      "Epoch 440:  Loss is 9.621099204165412e-22 \n",
      "Epoch 441:  Loss is 8.667543462915224e-22 \n",
      "Epoch 442:  Loss is 7.808323662861861e-22 \n",
      "Epoch 443:  Loss is 7.034351103244305e-22 \n",
      "Epoch 444:  Loss is 6.337008403908249e-22 \n",
      "Epoch 445:  Loss is 5.70884015947263e-22 \n",
      "Epoch 446:  Loss is 5.143012857965328e-22 \n",
      "Epoch 447:  Loss is 4.633338424130365e-22 \n",
      "Epoch 448:  Loss is 4.174091516540401e-22 \n",
      "Epoch 449:  Loss is 3.7603121352981934e-22 \n",
      "Epoch 450:  Loss is 3.38760606248577e-22 \n",
      "Epoch 451:  Loss is 3.0518344665840743e-22 \n",
      "Epoch 452:  Loss is 2.7493814676935704e-22 \n",
      "Epoch 453:  Loss is 2.476860066323447e-22 \n",
      "Epoch 454:  Loss is 2.231322714279567e-22 \n",
      "Epoch 455:  Loss is 2.010159466632582e-22 \n",
      "Epoch 456:  Loss is 1.8109376122813856e-22 \n",
      "Epoch 457:  Loss is 1.6315030195446939e-22 \n",
      "Epoch 458:  Loss is 1.4697900294318487e-22 \n",
      "Epoch 459:  Loss is 1.3241220077132622e-22 \n",
      "Epoch 460:  Loss is 1.1928937933658507e-22 \n",
      "Epoch 461:  Loss is 1.0746370535715726e-22 \n",
      "Epoch 462:  Loss is 9.681034590961423e-23 \n",
      "Epoch 463:  Loss is 8.72167086284715e-23 \n",
      "Epoch 464:  Loss is 7.857098102698235e-23 \n",
      "Epoch 465:  Loss is 7.078392079344066e-23 \n",
      "Epoch 466:  Loss is 6.376772486525178e-23 \n",
      "Epoch 467:  Loss is 5.744843786806363e-23 \n",
      "Epoch 468:  Loss is 5.175382266444378e-23 \n",
      "Epoch 469:  Loss is 4.6623422695780024e-23 \n",
      "Epoch 470:  Loss is 4.2003103076402645e-23 \n",
      "Epoch 471:  Loss is 3.7841856690036176e-23 \n",
      "Epoch 472:  Loss is 3.4091708223391184e-23 \n",
      "Epoch 473:  Loss is 3.0711643877749674e-23 \n",
      "Epoch 474:  Loss is 2.766759636458417e-23 \n",
      "Epoch 475:  Loss is 2.492335293307498e-23 \n",
      "Epoch 476:  Loss is 2.2452815576114275e-23 \n",
      "Epoch 477:  Loss is 2.0227650890954065e-23 \n",
      "Epoch 478:  Loss is 1.822343908997154e-23 \n",
      "Epoch 479:  Loss is 1.6415868405253716e-23 \n",
      "Epoch 480:  Loss is 1.479009253964536e-23 \n",
      "Epoch 481:  Loss is 1.332385135498623e-23 \n",
      "Epoch 482:  Loss is 1.200230306147508e-23 \n",
      "Epoch 483:  Loss is 1.0812175556819319e-23 \n",
      "Epoch 484:  Loss is 9.740874989992783e-24 \n",
      "Epoch 485:  Loss is 8.777957635908595e-24 \n",
      "Epoch 486:  Loss is 7.909271952141128e-24 \n",
      "Epoch 487:  Loss is 7.125864613935155e-24 \n",
      "Epoch 488:  Loss is 6.41921137647073e-24 \n",
      "Epoch 489:  Loss is 5.7826137858829064e-24 \n",
      "Epoch 490:  Loss is 5.2091341536647575e-24 \n",
      "Epoch 491:  Loss is 4.693104442721753e-24 \n",
      "Epoch 492:  Loss is 4.227546780465492e-24 \n",
      "Epoch 493:  Loss is 3.8087347043366955e-24 \n",
      "Epoch 494:  Loss is 3.430666414629174e-24 \n",
      "Epoch 495:  Loss is 3.090833406387439e-24 \n",
      "Epoch 496:  Loss is 2.7844658962430208e-24 \n",
      "Epoch 497:  Loss is 2.5082325433286173e-24 \n",
      "Epoch 498:  Loss is 2.2594088365491536e-24 \n",
      "Epoch 499:  Loss is 2.0353150509083347e-24 \n",
      "Epoch 500:  Loss is 1.8333779522086522e-24 \n",
      "Epoch 501:  Loss is 1.6511341011459803e-24 \n",
      "Epoch 502:  Loss is 1.4871304852047506e-24 \n",
      "Epoch 503:  Loss is 1.3396463105699262e-24 \n",
      "Epoch 504:  Loss is 1.2063182700242344e-24 \n",
      "Epoch 505:  Loss is 1.0873397572559325e-24 \n",
      "Epoch 506:  Loss is 9.803168454117112e-25 \n",
      "Epoch 507:  Loss is 8.830855120278837e-25 \n",
      "Epoch 508:  Loss is 7.9524485380159305e-25 \n",
      "Epoch 509:  Loss is 7.162523957402223e-25 \n",
      "Epoch 510:  Loss is 6.451630299707624e-25 \n",
      "Epoch 511:  Loss is 5.808155357378767e-25 \n",
      "Epoch 512:  Loss is 5.230081695071451e-25 \n",
      "Epoch 513:  Loss is 4.709618623558516e-25 \n",
      "Epoch 514:  Loss is 4.243070361546476e-25 \n",
      "Epoch 515:  Loss is 3.823055594244555e-25 \n",
      "Epoch 516:  Loss is 3.4454464048205776e-25 \n",
      "Epoch 517:  Loss is 3.10491437066047e-25 \n",
      "Epoch 518:  Loss is 2.7952726750865236e-25 \n",
      "Epoch 519:  Loss is 2.516129135899707e-25 \n",
      "Epoch 520:  Loss is 2.264415929738994e-25 \n",
      "Epoch 521:  Loss is 2.0392515267295128e-25 \n",
      "Epoch 522:  Loss is 1.8351507452693705e-25 \n",
      "Epoch 523:  Loss is 1.6536621242458968e-25 \n",
      "Epoch 524:  Loss is 1.489601671616003e-25 \n",
      "Epoch 525:  Loss is 1.3423890224117176e-25 \n",
      "Epoch 526:  Loss is 1.209442567690947e-25 \n",
      "Epoch 527:  Loss is 1.0905015815289446e-25 \n",
      "Epoch 528:  Loss is 9.811862416197842e-26 \n",
      "Epoch 529:  Loss is 8.832041754434754e-26 \n",
      "Epoch 530:  Loss is 7.948129421021852e-26 \n",
      "Epoch 531:  Loss is 7.155956507942173e-26 \n",
      "Epoch 532:  Loss is 6.435033191347313e-26 \n",
      "Epoch 533:  Loss is 5.806328306218469e-26 \n",
      "Epoch 534:  Loss is 5.229662750766211e-26 \n",
      "Epoch 535:  Loss is 4.711890937396005e-26 \n",
      "Epoch 536:  Loss is 4.2476832725283717e-26 \n",
      "Epoch 537:  Loss is 3.8296616633762954e-26 \n",
      "Epoch 538:  Loss is 3.4547958486837886e-26 \n",
      "Epoch 539:  Loss is 3.113060145902592e-26 \n",
      "Epoch 540:  Loss is 2.8028081283677986e-26 \n",
      "Epoch 541:  Loss is 2.5231628292718355e-26 \n",
      "Epoch 542:  Loss is 2.276116935913195e-26 \n",
      "Epoch 543:  Loss is 2.0519388136460372e-26 \n",
      "Epoch 544:  Loss is 1.859795075492837e-26 \n",
      "Epoch 545:  Loss is 1.684751672397339e-26 \n",
      "Epoch 546:  Loss is 1.518654543612726e-26 \n",
      "Epoch 547:  Loss is 1.369626491272446e-26 \n",
      "Epoch 548:  Loss is 1.2352324003696946e-26 \n",
      "Epoch 549:  Loss is 1.1148071872056608e-26 \n",
      "Epoch 550:  Loss is 1.0029362337864239e-26 \n",
      "Epoch 551:  Loss is 9.021454480785982e-27 \n",
      "Epoch 552:  Loss is 8.098883131244206e-27 \n",
      "Epoch 553:  Loss is 7.264996757262541e-27 \n",
      "Epoch 554:  Loss is 6.522623918224269e-27 \n",
      "Epoch 555:  Loss is 5.815310276485315e-27 \n",
      "Epoch 556:  Loss is 5.1906798579319364e-27 \n",
      "Epoch 557:  Loss is 4.635834293725705e-27 \n",
      "Epoch 558:  Loss is 4.135026441084212e-27 \n",
      "Epoch 559:  Loss is 3.6882469322842066e-27 \n",
      "Epoch 560:  Loss is 3.2984833314851815e-27 \n",
      "Epoch 561:  Loss is 2.9502867839345148e-27 \n",
      "Epoch 562:  Loss is 2.6410025261671042e-27 \n",
      "Epoch 563:  Loss is 2.3598640859408814e-27 \n",
      "Epoch 564:  Loss is 2.121425453919107e-27 \n",
      "Epoch 565:  Loss is 1.9036922019880882e-27 \n",
      "Epoch 566:  Loss is 1.7045965285000147e-27 \n",
      "Epoch 567:  Loss is 1.5321355108815645e-27 \n",
      "Epoch 568:  Loss is 1.37630343462614e-27 \n",
      "Epoch 569:  Loss is 1.244343028919802e-27 \n",
      "Epoch 570:  Loss is 1.1248180293081418e-27 \n",
      "Epoch 571:  Loss is 1.0188974290450836e-27 \n",
      "Epoch 572:  Loss is 9.241485783141522e-28 \n",
      "Epoch 573:  Loss is 8.293415490914608e-28 \n",
      "Epoch 574:  Loss is 7.411722913481386e-28 \n",
      "Epoch 575:  Loss is 6.651113089428601e-28 \n",
      "Epoch 576:  Loss is 5.87078667312595e-28 \n",
      "Epoch 577:  Loss is 5.218329679178966e-28 \n",
      "Epoch 578:  Loss is 4.642922208959116e-28 \n",
      "Epoch 579:  Loss is 4.1400603597356535e-28 \n",
      "Epoch 580:  Loss is 3.6825728037043716e-28 \n",
      "Epoch 581:  Loss is 3.2518079108374514e-28 \n",
      "Epoch 582:  Loss is 2.8348431534312415e-28 \n",
      "Epoch 583:  Loss is 2.515125224116152e-28 \n",
      "Epoch 584:  Loss is 2.20896337641922e-28 \n",
      "Epoch 585:  Loss is 1.9426587259585789e-28 \n",
      "Epoch 586:  Loss is 1.6757303823447478e-28 \n",
      "Epoch 587:  Loss is 1.4664628405219152e-28 \n",
      "Epoch 588:  Loss is 1.2905443934673006e-28 \n",
      "Epoch 589:  Loss is 1.1249896065550273e-28 \n",
      "Epoch 590:  Loss is 1.0097493542538815e-28 \n",
      "Epoch 591:  Loss is 9.059056768428506e-29 \n",
      "Epoch 592:  Loss is 8.244163453335201e-29 \n",
      "Epoch 593:  Loss is 7.506923633599589e-29 \n",
      "Epoch 594:  Loss is 6.704504181570091e-29 \n",
      "Epoch 595:  Loss is 6.228377321462635e-29 \n",
      "Epoch 596:  Loss is 5.882905548782407e-29 \n",
      "Epoch 597:  Loss is 5.518353202957148e-29 \n",
      "Epoch 598:  Loss is 5.274003537564939e-29 \n",
      "Epoch 599:  Loss is 4.9658793983662695e-29 \n",
      "Epoch 600:  Loss is 4.8089453820338643e-29 \n",
      "Epoch 601:  Loss is 4.550593435573983e-29 \n",
      "Epoch 602:  Loss is 4.399181445578125e-29 \n",
      "Epoch 603:  Loss is 4.358604412765819e-29 \n",
      "Epoch 604:  Loss is 4.241458568340499e-29 \n",
      "Epoch 605:  Loss is 4.084006862039043e-29 \n",
      "Epoch 606:  Loss is 3.9644697829947713e-29 \n",
      "Epoch 607:  Loss is 3.8792974571341903e-29 \n",
      "Epoch 608:  Loss is 3.8072645957261966e-29 \n",
      "Epoch 609:  Loss is 3.7987350371884943e-29 \n",
      "Epoch 610:  Loss is 3.7178767944033407e-29 \n",
      "Epoch 611:  Loss is 3.738830912198274e-29 \n",
      "Epoch 612:  Loss is 3.670520488186792e-29 \n",
      "Epoch 613:  Loss is 3.6150537057884394e-29 \n",
      "Epoch 614:  Loss is 3.6195403521868837e-29 \n",
      "Epoch 615:  Loss is 3.6146099715292524e-29 \n",
      "Epoch 616:  Loss is 3.6176175037304076e-29 \n",
      "Epoch 617:  Loss is 3.630584404859978e-29 \n",
      "Epoch 618:  Loss is 3.6708163110262494e-29 \n",
      "Epoch 619:  Loss is 3.666650139370551e-29 \n",
      "Epoch 620:  Loss is 3.620477124511834e-29 \n",
      "Epoch 621:  Loss is 3.666773398886992e-29 \n",
      "Epoch 622:  Loss is 3.668523684020451e-29 \n",
      "Epoch 623:  Loss is 3.6524506430765726e-29 \n",
      "Epoch 624:  Loss is 3.62493911900699e-29 \n",
      "Epoch 625:  Loss is 3.62493911900699e-29 \n",
      "Epoch 626:  Loss is 3.62493911900699e-29 \n",
      "Epoch 627:  Loss is 3.62493911900699e-29 \n",
      "Epoch 628:  Loss is 3.62493911900699e-29 \n",
      "Epoch 629:  Loss is 3.62493911900699e-29 \n",
      "Epoch 630:  Loss is 3.62493911900699e-29 \n",
      "Epoch 631:  Loss is 3.62493911900699e-29 \n",
      "Epoch 632:  Loss is 3.62493911900699e-29 \n",
      "Epoch 633:  Loss is 3.62493911900699e-29 \n",
      "Epoch 634:  Loss is 3.62493911900699e-29 \n",
      "Epoch 635:  Loss is 3.62493911900699e-29 \n",
      "Epoch 636:  Loss is 3.62493911900699e-29 \n",
      "Epoch 637:  Loss is 3.62493911900699e-29 \n",
      "Epoch 638:  Loss is 3.62493911900699e-29 \n",
      "Epoch 639:  Loss is 3.62493911900699e-29 \n",
      "Epoch 640:  Loss is 3.62493911900699e-29 \n",
      "Epoch 641:  Loss is 3.62493911900699e-29 \n",
      "Epoch 642:  Loss is 3.62493911900699e-29 \n",
      "Epoch 643:  Loss is 3.62493911900699e-29 \n",
      "Epoch 644:  Loss is 3.62493911900699e-29 \n",
      "Epoch 645:  Loss is 3.62493911900699e-29 \n",
      "Epoch 646:  Loss is 3.62493911900699e-29 \n",
      "Epoch 647:  Loss is 3.62493911900699e-29 \n",
      "Epoch 648:  Loss is 3.62493911900699e-29 \n",
      "Epoch 649:  Loss is 3.62493911900699e-29 \n",
      "Epoch 650:  Loss is 3.62493911900699e-29 \n",
      "Epoch 651:  Loss is 3.62493911900699e-29 \n",
      "Epoch 652:  Loss is 3.62493911900699e-29 \n",
      "Epoch 653:  Loss is 3.62493911900699e-29 \n",
      "Epoch 654:  Loss is 3.62493911900699e-29 \n",
      "Epoch 655:  Loss is 3.62493911900699e-29 \n",
      "Epoch 656:  Loss is 3.62493911900699e-29 \n",
      "Epoch 657:  Loss is 3.62493911900699e-29 \n",
      "Epoch 658:  Loss is 3.62493911900699e-29 \n",
      "Epoch 659:  Loss is 3.62493911900699e-29 \n",
      "Epoch 660:  Loss is 3.62493911900699e-29 \n",
      "Epoch 661:  Loss is 3.62493911900699e-29 \n",
      "Epoch 662:  Loss is 3.62493911900699e-29 \n",
      "Epoch 663:  Loss is 3.62493911900699e-29 \n",
      "Epoch 664:  Loss is 3.62493911900699e-29 \n",
      "Epoch 665:  Loss is 3.62493911900699e-29 \n",
      "Epoch 666:  Loss is 3.62493911900699e-29 \n",
      "Epoch 667:  Loss is 3.62493911900699e-29 \n",
      "Epoch 668:  Loss is 3.62493911900699e-29 \n",
      "Epoch 669:  Loss is 3.62493911900699e-29 \n",
      "Epoch 670:  Loss is 3.62493911900699e-29 \n",
      "Epoch 671:  Loss is 3.62493911900699e-29 \n",
      "Epoch 672:  Loss is 3.62493911900699e-29 \n",
      "Epoch 673:  Loss is 3.62493911900699e-29 \n",
      "Epoch 674:  Loss is 3.62493911900699e-29 \n",
      "Epoch 675:  Loss is 3.62493911900699e-29 \n",
      "Epoch 676:  Loss is 3.62493911900699e-29 \n",
      "Epoch 677:  Loss is 3.62493911900699e-29 \n",
      "Epoch 678:  Loss is 3.62493911900699e-29 \n",
      "Epoch 679:  Loss is 3.62493911900699e-29 \n",
      "Epoch 680:  Loss is 3.62493911900699e-29 \n",
      "Epoch 681:  Loss is 3.62493911900699e-29 \n",
      "Epoch 682:  Loss is 3.62493911900699e-29 \n",
      "Epoch 683:  Loss is 3.62493911900699e-29 \n",
      "Epoch 684:  Loss is 3.62493911900699e-29 \n",
      "Epoch 685:  Loss is 3.62493911900699e-29 \n",
      "Epoch 686:  Loss is 3.62493911900699e-29 \n",
      "Epoch 687:  Loss is 3.62493911900699e-29 \n",
      "Epoch 688:  Loss is 3.62493911900699e-29 \n",
      "Epoch 689:  Loss is 3.62493911900699e-29 \n",
      "Epoch 690:  Loss is 3.62493911900699e-29 \n",
      "Epoch 691:  Loss is 3.62493911900699e-29 \n",
      "Epoch 692:  Loss is 3.62493911900699e-29 \n",
      "Epoch 693:  Loss is 3.62493911900699e-29 \n",
      "Epoch 694:  Loss is 3.62493911900699e-29 \n",
      "Epoch 695:  Loss is 3.62493911900699e-29 \n",
      "Epoch 696:  Loss is 3.62493911900699e-29 \n",
      "Epoch 697:  Loss is 3.62493911900699e-29 \n",
      "Epoch 698:  Loss is 3.62493911900699e-29 \n",
      "Epoch 699:  Loss is 3.62493911900699e-29 \n",
      "Epoch 700:  Loss is 3.62493911900699e-29 \n",
      "Epoch 701:  Loss is 3.62493911900699e-29 \n",
      "Epoch 702:  Loss is 3.62493911900699e-29 \n",
      "Epoch 703:  Loss is 3.62493911900699e-29 \n",
      "Epoch 704:  Loss is 3.62493911900699e-29 \n",
      "Epoch 705:  Loss is 3.62493911900699e-29 \n",
      "Epoch 706:  Loss is 3.62493911900699e-29 \n",
      "Epoch 707:  Loss is 3.62493911900699e-29 \n",
      "Epoch 708:  Loss is 3.62493911900699e-29 \n",
      "Epoch 709:  Loss is 3.62493911900699e-29 \n",
      "Epoch 710:  Loss is 3.62493911900699e-29 \n",
      "Epoch 711:  Loss is 3.62493911900699e-29 \n",
      "Epoch 712:  Loss is 3.62493911900699e-29 \n",
      "Epoch 713:  Loss is 3.62493911900699e-29 \n",
      "Epoch 714:  Loss is 3.62493911900699e-29 \n",
      "Epoch 715:  Loss is 3.62493911900699e-29 \n",
      "Epoch 716:  Loss is 3.62493911900699e-29 \n",
      "Epoch 717:  Loss is 3.62493911900699e-29 \n",
      "Epoch 718:  Loss is 3.62493911900699e-29 \n",
      "Epoch 719:  Loss is 3.62493911900699e-29 \n",
      "Epoch 720:  Loss is 3.62493911900699e-29 \n",
      "Epoch 721:  Loss is 3.62493911900699e-29 \n",
      "Epoch 722:  Loss is 3.62493911900699e-29 \n",
      "Epoch 723:  Loss is 3.62493911900699e-29 \n",
      "Epoch 724:  Loss is 3.62493911900699e-29 \n",
      "Epoch 725:  Loss is 3.62493911900699e-29 \n",
      "Epoch 726:  Loss is 3.62493911900699e-29 \n",
      "Epoch 727:  Loss is 3.62493911900699e-29 \n",
      "Epoch 728:  Loss is 3.62493911900699e-29 \n",
      "Epoch 729:  Loss is 3.62493911900699e-29 \n",
      "Epoch 730:  Loss is 3.62493911900699e-29 \n",
      "Epoch 731:  Loss is 3.62493911900699e-29 \n",
      "Epoch 732:  Loss is 3.62493911900699e-29 \n",
      "Epoch 733:  Loss is 3.62493911900699e-29 \n",
      "Epoch 734:  Loss is 3.62493911900699e-29 \n",
      "Epoch 735:  Loss is 3.62493911900699e-29 \n",
      "Epoch 736:  Loss is 3.62493911900699e-29 \n",
      "Epoch 737:  Loss is 3.62493911900699e-29 \n",
      "Epoch 738:  Loss is 3.62493911900699e-29 \n",
      "Epoch 739:  Loss is 3.62493911900699e-29 \n",
      "Epoch 740:  Loss is 3.62493911900699e-29 \n",
      "Epoch 741:  Loss is 3.62493911900699e-29 \n",
      "Epoch 742:  Loss is 3.62493911900699e-29 \n",
      "Epoch 743:  Loss is 3.62493911900699e-29 \n",
      "Epoch 744:  Loss is 3.62493911900699e-29 \n",
      "Epoch 745:  Loss is 3.62493911900699e-29 \n",
      "Epoch 746:  Loss is 3.62493911900699e-29 \n",
      "Epoch 747:  Loss is 3.62493911900699e-29 \n",
      "Epoch 748:  Loss is 3.62493911900699e-29 \n",
      "Epoch 749:  Loss is 3.62493911900699e-29 \n",
      "Epoch 750:  Loss is 3.62493911900699e-29 \n",
      "Epoch 751:  Loss is 3.62493911900699e-29 \n",
      "Epoch 752:  Loss is 3.62493911900699e-29 \n",
      "Epoch 753:  Loss is 3.62493911900699e-29 \n",
      "Epoch 754:  Loss is 3.62493911900699e-29 \n",
      "Epoch 755:  Loss is 3.62493911900699e-29 \n",
      "Epoch 756:  Loss is 3.62493911900699e-29 \n",
      "Epoch 757:  Loss is 3.62493911900699e-29 \n",
      "Epoch 758:  Loss is 3.62493911900699e-29 \n",
      "Epoch 759:  Loss is 3.62493911900699e-29 \n",
      "Epoch 760:  Loss is 3.62493911900699e-29 \n",
      "Epoch 761:  Loss is 3.62493911900699e-29 \n",
      "Epoch 762:  Loss is 3.62493911900699e-29 \n",
      "Epoch 763:  Loss is 3.62493911900699e-29 \n",
      "Epoch 764:  Loss is 3.62493911900699e-29 \n",
      "Epoch 765:  Loss is 3.62493911900699e-29 \n",
      "Epoch 766:  Loss is 3.62493911900699e-29 \n",
      "Epoch 767:  Loss is 3.62493911900699e-29 \n",
      "Epoch 768:  Loss is 3.62493911900699e-29 \n",
      "Epoch 769:  Loss is 3.62493911900699e-29 \n",
      "Epoch 770:  Loss is 3.62493911900699e-29 \n",
      "Epoch 771:  Loss is 3.62493911900699e-29 \n",
      "Epoch 772:  Loss is 3.62493911900699e-29 \n",
      "Epoch 773:  Loss is 3.62493911900699e-29 \n",
      "Epoch 774:  Loss is 3.62493911900699e-29 \n",
      "Epoch 775:  Loss is 3.62493911900699e-29 \n",
      "Epoch 776:  Loss is 3.62493911900699e-29 \n",
      "Epoch 777:  Loss is 3.62493911900699e-29 \n",
      "Epoch 778:  Loss is 3.62493911900699e-29 \n",
      "Epoch 779:  Loss is 3.62493911900699e-29 \n",
      "Epoch 780:  Loss is 3.62493911900699e-29 \n",
      "Epoch 781:  Loss is 3.62493911900699e-29 \n",
      "Epoch 782:  Loss is 3.62493911900699e-29 \n",
      "Epoch 783:  Loss is 3.62493911900699e-29 \n",
      "Epoch 784:  Loss is 3.62493911900699e-29 \n",
      "Epoch 785:  Loss is 3.62493911900699e-29 \n",
      "Epoch 786:  Loss is 3.62493911900699e-29 \n",
      "Epoch 787:  Loss is 3.62493911900699e-29 \n",
      "Epoch 788:  Loss is 3.62493911900699e-29 \n",
      "Epoch 789:  Loss is 3.62493911900699e-29 \n",
      "Epoch 790:  Loss is 3.62493911900699e-29 \n",
      "Epoch 791:  Loss is 3.62493911900699e-29 \n",
      "Epoch 792:  Loss is 3.62493911900699e-29 \n",
      "Epoch 793:  Loss is 3.62493911900699e-29 \n",
      "Epoch 794:  Loss is 3.62493911900699e-29 \n",
      "Epoch 795:  Loss is 3.62493911900699e-29 \n",
      "Epoch 796:  Loss is 3.62493911900699e-29 \n",
      "Epoch 797:  Loss is 3.62493911900699e-29 \n",
      "Epoch 798:  Loss is 3.62493911900699e-29 \n",
      "Epoch 799:  Loss is 3.62493911900699e-29 \n",
      "Epoch 800:  Loss is 3.62493911900699e-29 \n",
      "Epoch 801:  Loss is 3.62493911900699e-29 \n",
      "Epoch 802:  Loss is 3.62493911900699e-29 \n",
      "Epoch 803:  Loss is 3.62493911900699e-29 \n",
      "Epoch 804:  Loss is 3.62493911900699e-29 \n",
      "Epoch 805:  Loss is 3.62493911900699e-29 \n",
      "Epoch 806:  Loss is 3.62493911900699e-29 \n",
      "Epoch 807:  Loss is 3.62493911900699e-29 \n",
      "Epoch 808:  Loss is 3.62493911900699e-29 \n",
      "Epoch 809:  Loss is 3.62493911900699e-29 \n",
      "Epoch 810:  Loss is 3.62493911900699e-29 \n",
      "Epoch 811:  Loss is 3.62493911900699e-29 \n",
      "Epoch 812:  Loss is 3.62493911900699e-29 \n",
      "Epoch 813:  Loss is 3.62493911900699e-29 \n",
      "Epoch 814:  Loss is 3.62493911900699e-29 \n",
      "Epoch 815:  Loss is 3.62493911900699e-29 \n",
      "Epoch 816:  Loss is 3.62493911900699e-29 \n",
      "Epoch 817:  Loss is 3.62493911900699e-29 \n",
      "Epoch 818:  Loss is 3.62493911900699e-29 \n",
      "Epoch 819:  Loss is 3.62493911900699e-29 \n",
      "Epoch 820:  Loss is 3.62493911900699e-29 \n",
      "Epoch 821:  Loss is 3.62493911900699e-29 \n",
      "Epoch 822:  Loss is 3.62493911900699e-29 \n",
      "Epoch 823:  Loss is 3.62493911900699e-29 \n",
      "Epoch 824:  Loss is 3.62493911900699e-29 \n",
      "Epoch 825:  Loss is 3.62493911900699e-29 \n",
      "Epoch 826:  Loss is 3.62493911900699e-29 \n",
      "Epoch 827:  Loss is 3.62493911900699e-29 \n",
      "Epoch 828:  Loss is 3.62493911900699e-29 \n",
      "Epoch 829:  Loss is 3.62493911900699e-29 \n",
      "Epoch 830:  Loss is 3.62493911900699e-29 \n",
      "Epoch 831:  Loss is 3.62493911900699e-29 \n",
      "Epoch 832:  Loss is 3.62493911900699e-29 \n",
      "Epoch 833:  Loss is 3.62493911900699e-29 \n",
      "Epoch 834:  Loss is 3.62493911900699e-29 \n",
      "Epoch 835:  Loss is 3.62493911900699e-29 \n",
      "Epoch 836:  Loss is 3.62493911900699e-29 \n",
      "Epoch 837:  Loss is 3.62493911900699e-29 \n",
      "Epoch 838:  Loss is 3.62493911900699e-29 \n",
      "Epoch 839:  Loss is 3.62493911900699e-29 \n",
      "Epoch 840:  Loss is 3.62493911900699e-29 \n",
      "Epoch 841:  Loss is 3.62493911900699e-29 \n",
      "Epoch 842:  Loss is 3.62493911900699e-29 \n",
      "Epoch 843:  Loss is 3.62493911900699e-29 \n",
      "Epoch 844:  Loss is 3.62493911900699e-29 \n",
      "Epoch 845:  Loss is 3.62493911900699e-29 \n",
      "Epoch 846:  Loss is 3.62493911900699e-29 \n",
      "Epoch 847:  Loss is 3.62493911900699e-29 \n",
      "Epoch 848:  Loss is 3.62493911900699e-29 \n",
      "Epoch 849:  Loss is 3.62493911900699e-29 \n",
      "Epoch 850:  Loss is 3.62493911900699e-29 \n",
      "Epoch 851:  Loss is 3.62493911900699e-29 \n",
      "Epoch 852:  Loss is 3.62493911900699e-29 \n",
      "Epoch 853:  Loss is 3.62493911900699e-29 \n",
      "Epoch 854:  Loss is 3.62493911900699e-29 \n",
      "Epoch 855:  Loss is 3.62493911900699e-29 \n",
      "Epoch 856:  Loss is 3.62493911900699e-29 \n",
      "Epoch 857:  Loss is 3.62493911900699e-29 \n",
      "Epoch 858:  Loss is 3.62493911900699e-29 \n",
      "Epoch 859:  Loss is 3.62493911900699e-29 \n",
      "Epoch 860:  Loss is 3.62493911900699e-29 \n",
      "Epoch 861:  Loss is 3.62493911900699e-29 \n",
      "Epoch 862:  Loss is 3.62493911900699e-29 \n",
      "Epoch 863:  Loss is 3.62493911900699e-29 \n",
      "Epoch 864:  Loss is 3.62493911900699e-29 \n",
      "Epoch 865:  Loss is 3.62493911900699e-29 \n",
      "Epoch 866:  Loss is 3.62493911900699e-29 \n",
      "Epoch 867:  Loss is 3.62493911900699e-29 \n",
      "Epoch 868:  Loss is 3.62493911900699e-29 \n",
      "Epoch 869:  Loss is 3.62493911900699e-29 \n",
      "Epoch 870:  Loss is 3.62493911900699e-29 \n",
      "Epoch 871:  Loss is 3.62493911900699e-29 \n",
      "Epoch 872:  Loss is 3.62493911900699e-29 \n",
      "Epoch 873:  Loss is 3.62493911900699e-29 \n",
      "Epoch 874:  Loss is 3.62493911900699e-29 \n",
      "Epoch 875:  Loss is 3.62493911900699e-29 \n",
      "Epoch 876:  Loss is 3.62493911900699e-29 \n",
      "Epoch 877:  Loss is 3.62493911900699e-29 \n",
      "Epoch 878:  Loss is 3.62493911900699e-29 \n",
      "Epoch 879:  Loss is 3.62493911900699e-29 \n",
      "Epoch 880:  Loss is 3.62493911900699e-29 \n",
      "Epoch 881:  Loss is 3.62493911900699e-29 \n",
      "Epoch 882:  Loss is 3.62493911900699e-29 \n",
      "Epoch 883:  Loss is 3.62493911900699e-29 \n",
      "Epoch 884:  Loss is 3.62493911900699e-29 \n",
      "Epoch 885:  Loss is 3.62493911900699e-29 \n",
      "Epoch 886:  Loss is 3.62493911900699e-29 \n",
      "Epoch 887:  Loss is 3.62493911900699e-29 \n",
      "Epoch 888:  Loss is 3.62493911900699e-29 \n",
      "Epoch 889:  Loss is 3.62493911900699e-29 \n",
      "Epoch 890:  Loss is 3.62493911900699e-29 \n",
      "Epoch 891:  Loss is 3.62493911900699e-29 \n",
      "Epoch 892:  Loss is 3.62493911900699e-29 \n",
      "Epoch 893:  Loss is 3.62493911900699e-29 \n",
      "Epoch 894:  Loss is 3.62493911900699e-29 \n",
      "Epoch 895:  Loss is 3.62493911900699e-29 \n",
      "Epoch 896:  Loss is 3.62493911900699e-29 \n",
      "Epoch 897:  Loss is 3.62493911900699e-29 \n",
      "Epoch 898:  Loss is 3.62493911900699e-29 \n",
      "Epoch 899:  Loss is 3.62493911900699e-29 \n",
      "Epoch 900:  Loss is 3.62493911900699e-29 \n",
      "Epoch 901:  Loss is 3.62493911900699e-29 \n",
      "Epoch 902:  Loss is 3.62493911900699e-29 \n",
      "Epoch 903:  Loss is 3.62493911900699e-29 \n",
      "Epoch 904:  Loss is 3.62493911900699e-29 \n",
      "Epoch 905:  Loss is 3.62493911900699e-29 \n",
      "Epoch 906:  Loss is 3.62493911900699e-29 \n",
      "Epoch 907:  Loss is 3.62493911900699e-29 \n",
      "Epoch 908:  Loss is 3.62493911900699e-29 \n",
      "Epoch 909:  Loss is 3.62493911900699e-29 \n",
      "Epoch 910:  Loss is 3.62493911900699e-29 \n",
      "Epoch 911:  Loss is 3.62493911900699e-29 \n",
      "Epoch 912:  Loss is 3.62493911900699e-29 \n",
      "Epoch 913:  Loss is 3.62493911900699e-29 \n",
      "Epoch 914:  Loss is 3.62493911900699e-29 \n",
      "Epoch 915:  Loss is 3.62493911900699e-29 \n",
      "Epoch 916:  Loss is 3.62493911900699e-29 \n",
      "Epoch 917:  Loss is 3.62493911900699e-29 \n",
      "Epoch 918:  Loss is 3.62493911900699e-29 \n",
      "Epoch 919:  Loss is 3.62493911900699e-29 \n",
      "Epoch 920:  Loss is 3.62493911900699e-29 \n",
      "Epoch 921:  Loss is 3.62493911900699e-29 \n",
      "Epoch 922:  Loss is 3.62493911900699e-29 \n",
      "Epoch 923:  Loss is 3.62493911900699e-29 \n",
      "Epoch 924:  Loss is 3.62493911900699e-29 \n",
      "Epoch 925:  Loss is 3.62493911900699e-29 \n",
      "Epoch 926:  Loss is 3.62493911900699e-29 \n",
      "Epoch 927:  Loss is 3.62493911900699e-29 \n",
      "Epoch 928:  Loss is 3.62493911900699e-29 \n",
      "Epoch 929:  Loss is 3.62493911900699e-29 \n",
      "Epoch 930:  Loss is 3.62493911900699e-29 \n",
      "Epoch 931:  Loss is 3.62493911900699e-29 \n",
      "Epoch 932:  Loss is 3.62493911900699e-29 \n",
      "Epoch 933:  Loss is 3.62493911900699e-29 \n",
      "Epoch 934:  Loss is 3.62493911900699e-29 \n",
      "Epoch 935:  Loss is 3.62493911900699e-29 \n",
      "Epoch 936:  Loss is 3.62493911900699e-29 \n",
      "Epoch 937:  Loss is 3.62493911900699e-29 \n",
      "Epoch 938:  Loss is 3.62493911900699e-29 \n",
      "Epoch 939:  Loss is 3.62493911900699e-29 \n",
      "Epoch 940:  Loss is 3.62493911900699e-29 \n",
      "Epoch 941:  Loss is 3.62493911900699e-29 \n",
      "Epoch 942:  Loss is 3.62493911900699e-29 \n",
      "Epoch 943:  Loss is 3.62493911900699e-29 \n",
      "Epoch 944:  Loss is 3.62493911900699e-29 \n",
      "Epoch 945:  Loss is 3.62493911900699e-29 \n",
      "Epoch 946:  Loss is 3.62493911900699e-29 \n",
      "Epoch 947:  Loss is 3.62493911900699e-29 \n",
      "Epoch 948:  Loss is 3.62493911900699e-29 \n",
      "Epoch 949:  Loss is 3.62493911900699e-29 \n",
      "Epoch 950:  Loss is 3.62493911900699e-29 \n",
      "Epoch 951:  Loss is 3.62493911900699e-29 \n",
      "Epoch 952:  Loss is 3.62493911900699e-29 \n",
      "Epoch 953:  Loss is 3.62493911900699e-29 \n",
      "Epoch 954:  Loss is 3.62493911900699e-29 \n",
      "Epoch 955:  Loss is 3.62493911900699e-29 \n",
      "Epoch 956:  Loss is 3.62493911900699e-29 \n",
      "Epoch 957:  Loss is 3.62493911900699e-29 \n",
      "Epoch 958:  Loss is 3.62493911900699e-29 \n",
      "Epoch 959:  Loss is 3.62493911900699e-29 \n",
      "Epoch 960:  Loss is 3.62493911900699e-29 \n",
      "Epoch 961:  Loss is 3.62493911900699e-29 \n",
      "Epoch 962:  Loss is 3.62493911900699e-29 \n",
      "Epoch 963:  Loss is 3.62493911900699e-29 \n",
      "Epoch 964:  Loss is 3.62493911900699e-29 \n",
      "Epoch 965:  Loss is 3.62493911900699e-29 \n",
      "Epoch 966:  Loss is 3.62493911900699e-29 \n",
      "Epoch 967:  Loss is 3.62493911900699e-29 \n",
      "Epoch 968:  Loss is 3.62493911900699e-29 \n",
      "Epoch 969:  Loss is 3.62493911900699e-29 \n",
      "Epoch 970:  Loss is 3.62493911900699e-29 \n",
      "Epoch 971:  Loss is 3.62493911900699e-29 \n",
      "Epoch 972:  Loss is 3.62493911900699e-29 \n",
      "Epoch 973:  Loss is 3.62493911900699e-29 \n",
      "Epoch 974:  Loss is 3.62493911900699e-29 \n",
      "Epoch 975:  Loss is 3.62493911900699e-29 \n",
      "Epoch 976:  Loss is 3.62493911900699e-29 \n",
      "Epoch 977:  Loss is 3.62493911900699e-29 \n",
      "Epoch 978:  Loss is 3.62493911900699e-29 \n",
      "Epoch 979:  Loss is 3.62493911900699e-29 \n",
      "Epoch 980:  Loss is 3.62493911900699e-29 \n",
      "Epoch 981:  Loss is 3.62493911900699e-29 \n",
      "Epoch 982:  Loss is 3.62493911900699e-29 \n",
      "Epoch 983:  Loss is 3.62493911900699e-29 \n",
      "Epoch 984:  Loss is 3.62493911900699e-29 \n",
      "Epoch 985:  Loss is 3.62493911900699e-29 \n",
      "Epoch 986:  Loss is 3.62493911900699e-29 \n",
      "Epoch 987:  Loss is 3.62493911900699e-29 \n",
      "Epoch 988:  Loss is 3.62493911900699e-29 \n",
      "Epoch 989:  Loss is 3.62493911900699e-29 \n",
      "Epoch 990:  Loss is 3.62493911900699e-29 \n",
      "Epoch 991:  Loss is 3.62493911900699e-29 \n",
      "Epoch 992:  Loss is 3.62493911900699e-29 \n",
      "Epoch 993:  Loss is 3.62493911900699e-29 \n",
      "Epoch 994:  Loss is 3.62493911900699e-29 \n",
      "Epoch 995:  Loss is 3.62493911900699e-29 \n",
      "Epoch 996:  Loss is 3.62493911900699e-29 \n",
      "Epoch 997:  Loss is 3.62493911900699e-29 \n",
      "Epoch 998:  Loss is 3.62493911900699e-29 \n",
      "Epoch 999:  Loss is 3.62493911900699e-29 \n",
      "\n",
      "\u001b[36m Mean Square Error is: 3.984738577878294e-30\u001b[0m\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Dense(5, activation_function=\"reLu\")\n",
    "\n",
    "b = NetModel(input_shape=(2, ), usage=\"LinearRegression\")\n",
    "#b.add_layer(a)\n",
    "#b.add_layer(Dense(2, activation_function=\"reLu\"))\n",
    "b.compile(activation_function=\"reLu\")\n",
    "\n",
    "X = np.vstack([np.random.random(100),np.random.random(100), np.ones(100)]).T\n",
    "y = 2*X[:, 0] + 3*X[:, 1] + 1\n",
    "\n",
    "b.train(X, y,\"l2\", nepochs= 1000, learning_rate=0.01)\n",
    "b.display_losses()\n",
    "\n",
    "X_p = np.vstack([np.random.random(100),np.random.random(100), np.ones(100)]).T\n",
    "y_p = 2*X_p[:, 0] + 3*X_p[:, 1] + 1\n",
    "h=b.predict_sample(X_p, y_p, \"MSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  Loss is 0.2698471001204068 \n",
      "Epoch 1:  Loss is 0.2698459107944285 \n",
      "Epoch 2:  Loss is 0.2698447038069045 \n",
      "Epoch 3:  Loss is 0.26984347876977893 \n",
      "Epoch 4:  Loss is 0.2698422352836756 \n",
      "Epoch 5:  Loss is 0.26984097293748593 \n",
      "Epoch 6:  Loss is 0.2698396913079372 \n",
      "Epoch 7:  Loss is 0.26983838995914333 \n",
      "Epoch 8:  Loss is 0.2698370684421351 \n",
      "Epoch 9:  Loss is 0.2698357262943689 \n",
      "Epoch 10:  Loss is 0.269834363039214 \n",
      "Epoch 11:  Loss is 0.26983297818541585 \n",
      "Epoch 12:  Loss is 0.2698315712265355 \n",
      "Epoch 13:  Loss is 0.2698301416403627 \n",
      "Epoch 14:  Loss is 0.2698286888883014 \n",
      "Epoch 15:  Loss is 0.26982721241472785 \n",
      "Epoch 16:  Loss is 0.2698257116463163 \n",
      "Epoch 17:  Loss is 0.2698241859913345 \n",
      "Epoch 18:  Loss is 0.26982263483890456 \n",
      "Epoch 19:  Loss is 0.26982105755822805 \n",
      "Epoch 20:  Loss is 0.2698194534977739 \n",
      "Epoch 21:  Loss is 0.26981782198442555 \n",
      "Epoch 22:  Loss is 0.2698161623225872 \n",
      "Epoch 23:  Loss is 0.26981447379324414 \n",
      "Epoch 24:  Loss is 0.26981275565297674 \n",
      "Epoch 25:  Loss is 0.269811007132924 \n",
      "Epoch 26:  Loss is 0.2698092274376946 \n",
      "Epoch 27:  Loss is 0.2698074157442212 \n",
      "Epoch 28:  Loss is 0.26980557120055565 \n",
      "Epoch 29:  Loss is 0.2698036929246009 \n",
      "Epoch 30:  Loss is 0.26980178000277594 \n",
      "Epoch 31:  Loss is 0.26979983148860937 \n",
      "Epoch 32:  Loss is 0.2697978464012577 \n",
      "Epoch 33:  Loss is 0.2697958237239428 \n",
      "Epoch 34:  Loss is 0.269793762402304 \n",
      "Epoch 35:  Loss is 0.26979166134265953 \n",
      "Epoch 36:  Loss is 0.26978951941017043 \n",
      "Epoch 37:  Loss is 0.2697873354269016 \n",
      "Epoch 38:  Loss is 0.2697851081697729 \n",
      "Epoch 39:  Loss is 0.26978283636839256 \n",
      "Epoch 40:  Loss is 0.26978051870276554 \n",
      "Epoch 41:  Loss is 0.269778153800868 \n",
      "Epoch 42:  Loss is 0.26977574023607914 \n",
      "Epoch 43:  Loss is 0.2697732765244599 \n",
      "Epoch 44:  Loss is 0.2697707611218686 \n",
      "Epoch 45:  Loss is 0.2697681924209009 \n",
      "Epoch 46:  Loss is 0.2697655687476433 \n",
      "Epoch 47:  Loss is 0.26976288835822504 \n",
      "Epoch 48:  Loss is 0.2697601494351548 \n",
      "Epoch 49:  Loss is 0.2697573500834258 \n",
      "Epoch 50:  Loss is 0.2697544883263729 \n",
      "Epoch 51:  Loss is 0.2697515621012623 \n",
      "Epoch 52:  Loss is 0.269748569254594 \n",
      "Epoch 53:  Loss is 0.2697455075370952 \n",
      "Epoch 54:  Loss is 0.26974237459838074 \n",
      "Epoch 55:  Loss is 0.26973916798125347 \n",
      "Epoch 56:  Loss is 0.2697358851156178 \n",
      "Epoch 57:  Loss is 0.26973252331197506 \n",
      "Epoch 58:  Loss is 0.2697290797544657 \n",
      "Epoch 59:  Loss is 0.26972555149342237 \n",
      "Epoch 60:  Loss is 0.26972193543739353 \n",
      "Epoch 61:  Loss is 0.2697182283445932 \n",
      "Epoch 62:  Loss is 0.26971442681372737 \n",
      "Epoch 63:  Loss is 0.26971052727414546 \n",
      "Epoch 64:  Loss is 0.2697065259752564 \n",
      "Epoch 65:  Loss is 0.26970241897514674 \n",
      "Epoch 66:  Loss is 0.26969820212832835 \n",
      "Epoch 67:  Loss is 0.2696938710725385 \n",
      "Epoch 68:  Loss is 0.26968942121450584 \n",
      "Epoch 69:  Loss is 0.2696848477145865 \n",
      "Epoch 70:  Loss is 0.2696801454701653 \n",
      "Epoch 71:  Loss is 0.2696753090977053 \n",
      "Epoch 72:  Loss is 0.26967033291331566 \n",
      "Epoch 73:  Loss is 0.2696652109116939 \n",
      "Epoch 74:  Loss is 0.26965993674328304 \n",
      "Epoch 75:  Loss is 0.2696545036894649 \n",
      "Epoch 76:  Loss is 0.26964890463559 \n",
      "Epoch 77:  Loss is 0.26964313204162343 \n",
      "Epoch 78:  Loss is 0.26963717791015723 \n",
      "Epoch 79:  Loss is 0.2696310337515107 \n",
      "Epoch 80:  Loss is 0.2696246905456065 \n",
      "Epoch 81:  Loss is 0.2696181387002717 \n",
      "Epoch 82:  Loss is 0.26961136800556773 \n",
      "Epoch 83:  Loss is 0.26960436758370343 \n",
      "Epoch 84:  Loss is 0.26959712583402734 \n",
      "Epoch 85:  Loss is 0.26958963037252826 \n",
      "Epoch 86:  Loss is 0.2695818679651977 \n",
      "Epoch 87:  Loss is 0.2695738244545181 \n",
      "Epoch 88:  Loss is 0.26956548467824043 \n",
      "Epoch 89:  Loss is 0.26955683237949346 \n",
      "Epoch 90:  Loss is 0.26954785010713567 \n",
      "Epoch 91:  Loss is 0.2695385191050933 \n",
      "Epoch 92:  Loss is 0.26952881918924865 \n",
      "Epoch 93:  Loss is 0.26951872861022136 \n",
      "Epoch 94:  Loss is 0.2695082239001286 \n",
      "Epoch 95:  Loss is 0.26949727970111403 \n",
      "Epoch 96:  Loss is 0.26948586857307444 \n",
      "Epoch 97:  Loss is 0.2694739607775972 \n",
      "Epoch 98:  Loss is 0.26946152403462237 \n",
      "Epoch 99:  Loss is 0.26944852324774643 \n",
      "Epoch 100:  Loss is 0.2694349201933805 \n",
      "Epoch 101:  Loss is 0.26942067316811785 \n",
      "Epoch 102:  Loss is 0.2694057365876472 \n",
      "Epoch 103:  Loss is 0.26939006052930664 \n",
      "Epoch 104:  Loss is 0.2693735902088728 \n",
      "Epoch 105:  Loss is 0.26935626538035157 \n",
      "Epoch 106:  Loss is 0.26933801964529763 \n",
      "Epoch 107:  Loss is 0.26931877965544304 \n",
      "Epoch 108:  Loss is 0.2692984641890192 \n",
      "Epoch 109:  Loss is 0.26927698307694514 \n",
      "Epoch 110:  Loss is 0.26925423594980513 \n",
      "Epoch 111:  Loss is 0.26923011076994924 \n",
      "Epoch 112:  Loss is 0.2692044821047499 \n",
      "Epoch 113:  Loss is 0.2691772090865076 \n",
      "Epoch 114:  Loss is 0.2691481329910514 \n",
      "Epoch 115:  Loss is 0.26911707434979326 \n",
      "Epoch 116:  Loss is 0.26908382948762594 \n",
      "Epoch 117:  Loss is 0.2690481663498871 \n",
      "Epoch 118:  Loss is 0.2690098194432893 \n",
      "Epoch 119:  Loss is 0.26896848366493264 \n",
      "Epoch 120:  Loss is 0.2689238067256332 \n",
      "Epoch 121:  Loss is 0.2688753797821954 \n",
      "Epoch 122:  Loss is 0.2688227257683942 \n",
      "Epoch 123:  Loss is 0.2687652847424019 \n",
      "Epoch 124:  Loss is 0.2687023953286313 \n",
      "Epoch 125:  Loss is 0.2686332709936517 \n",
      "Epoch 126:  Loss is 0.2685569694120487 \n",
      "Epoch 127:  Loss is 0.2684723524762831 \n",
      "Epoch 128:  Loss is 0.2683780334704714 \n",
      "Epoch 129:  Loss is 0.2682723063781868 \n",
      "Epoch 130:  Loss is 0.26815304992827854 \n",
      "Epoch 131:  Loss is 0.2680175952964534 \n",
      "Epoch 132:  Loss is 0.2678625405077403 \n",
      "Epoch 133:  Loss is 0.2676834849954415 \n",
      "Epoch 134:  Loss is 0.26747464167757384 \n",
      "Epoch 135:  Loss is 0.2672282560573964 \n",
      "Epoch 136:  Loss is 0.2669337119527059 \n",
      "Epoch 137:  Loss is 0.26657611048358165 \n",
      "Epoch 138:  Loss is 0.26613392777675665 \n",
      "Epoch 139:  Loss is 0.26557498505280125 \n",
      "Epoch 140:  Loss is 0.2648491542698942 \n",
      "Epoch 141:  Loss is 0.26387432503724867 \n",
      "Epoch 142:  Loss is 0.2625073211546909 \n",
      "Epoch 143:  Loss is 0.2604777863717309 \n",
      "Epoch 144:  Loss is 0.2572193256446907 \n",
      "Epoch 145:  Loss is 0.2513707600452529 \n",
      "Epoch 146:  Loss is 0.2390566594470754 \n",
      "Epoch 147:  Loss is 0.20857206129385983 \n",
      "Epoch 148:  Loss is 0.15350634862006496 \n",
      "Epoch 149:  Loss is 0.12486677479687879 \n",
      "Epoch 150:  Loss is 0.12077670846587829 \n",
      "Epoch 151:  Loss is 0.11983954954593624 \n",
      "Epoch 152:  Loss is 0.11909807104098684 \n",
      "Epoch 153:  Loss is 0.11833873120133377 \n",
      "Epoch 154:  Loss is 0.11754369729972014 \n",
      "Epoch 155:  Loss is 0.11670903198407567 \n",
      "Epoch 156:  Loss is 0.11583200897087961 \n",
      "Epoch 157:  Loss is 0.11491010387003817 \n",
      "Epoch 158:  Loss is 0.1139408947018352 \n",
      "Epoch 159:  Loss is 0.11292206428095791 \n",
      "Epoch 160:  Loss is 0.11185142276773172 \n",
      "Epoch 161:  Loss is 0.11072693736810138 \n",
      "Epoch 162:  Loss is 0.10954676630843499 \n",
      "Epoch 163:  Loss is 0.10830929619873361 \n",
      "Epoch 164:  Loss is 0.10701318222042171 \n",
      "Epoch 165:  Loss is 0.10565739049315953 \n",
      "Epoch 166:  Loss is 0.10424124178111445 \n",
      "Epoch 167:  Loss is 0.10276445546787022 \n",
      "Epoch 168:  Loss is 0.1012271924877439 \n",
      "Epoch 169:  Loss is 0.09963009566927195 \n",
      "Epoch 170:  Loss is 0.09797432574522619 \n",
      "Epoch 171:  Loss is 0.09626159113696593 \n",
      "Epoch 172:  Loss is 0.09449416955529191 \n",
      "Epoch 173:  Loss is 0.09267491950068829 \n",
      "Epoch 174:  Loss is 0.09080727991437364 \n",
      "Epoch 175:  Loss is 0.08889525654152373 \n",
      "Epoch 176:  Loss is 0.08694339402128287 \n",
      "Epoch 177:  Loss is 0.08495673330192786 \n",
      "Epoch 178:  Loss is 0.08294075466479818 \n",
      "Epoch 179:  Loss is 0.0809013073829822 \n",
      "Epoch 180:  Loss is 0.07884452778383388 \n",
      "Epoch 181:  Loss is 0.07677674816549565 \n",
      "Epoch 182:  Loss is 0.07470439957513712 \n",
      "Epoch 183:  Loss is 0.07263391183803995 \n",
      "Epoch 184:  Loss is 0.0705716143954573 \n",
      "Epoch 185:  Loss is 0.06852364144975862 \n",
      "Epoch 186:  Loss is 0.06649584463492783 \n",
      "Epoch 187:  Loss is 0.06449371595761257 \n",
      "Epoch 188:  Loss is 0.06252232313438082 \n",
      "Epoch 189:  Loss is 0.06058625874130199 \n",
      "Epoch 190:  Loss is 0.05868960385286788 \n",
      "Epoch 191:  Loss is 0.05683590613596024 \n",
      "Epoch 192:  Loss is 0.05502817172960287 \n",
      "Epoch 193:  Loss is 0.05326886971850817 \n",
      "Epoch 194:  Loss is 0.05155994761932475 \n",
      "Epoch 195:  Loss is 0.049902856050031155 \n",
      "Epoch 196:  Loss is 0.04829858063946891 \n",
      "Epoch 197:  Loss is 0.04674767924008662 \n",
      "Epoch 198:  Loss is 0.04525032261062137 \n",
      "Epoch 199:  Loss is 0.043806336911555416 \n",
      "Epoch 200:  Loss is 0.04241524657944423 \n",
      "Epoch 201:  Loss is 0.04107631639338219 \n",
      "Epoch 202:  Loss is 0.03978859179831427 \n",
      "Epoch 203:  Loss is 0.038550936790347996 \n",
      "Epoch 204:  Loss is 0.03736206888789882 \n",
      "Epoch 205:  Loss is 0.03622059090282804 \n",
      "Epoch 206:  Loss is 0.035125019384668076 \n",
      "Epoch 207:  Loss is 0.03407380973832283 \n",
      "Epoch 208:  Loss is 0.03306537811304147 \n",
      "Epoch 209:  Loss is 0.0320981202310367 \n",
      "Epoch 210:  Loss is 0.031170427371598225 \n",
      "Epoch 211:  Loss is 0.030280699754906087 \n",
      "Epoch 212:  Loss is 0.029427357582846557 \n",
      "Epoch 213:  Loss is 0.02860884999552043 \n",
      "Epoch 214:  Loss is 0.027823662194921904 \n",
      "Epoch 215:  Loss is 0.027070320974071835 \n",
      "Epoch 216:  Loss is 0.026347398872846442 \n",
      "Epoch 217:  Loss is 0.025653517162534875 \n",
      "Epoch 218:  Loss is 0.024987347841059614 \n",
      "Epoch 219:  Loss is 0.024347614800748384 \n",
      "Epoch 220:  Loss is 0.023733094311210734 \n",
      "Epoch 221:  Loss is 0.023142614941680635 \n",
      "Epoch 222:  Loss is 0.022575057030405455 \n",
      "Epoch 223:  Loss is 0.022029351793416294 \n",
      "Epoch 224:  Loss is 0.021504480151348083 \n",
      "Epoch 225:  Loss is 0.02099947134085268 \n",
      "Epoch 226:  Loss is 0.02051340136649665 \n",
      "Epoch 227:  Loss is 0.0200453913397486 \n",
      "Epoch 228:  Loss is 0.019594605743626245 \n",
      "Epoch 229:  Loss is 0.019160250654664205 \n",
      "Epoch 230:  Loss is 0.01874157194796068 \n",
      "Epoch 231:  Loss is 0.018337853506042827 \n",
      "Epoch 232:  Loss is 0.017948415448049978 \n",
      "Epoch 233:  Loss is 0.01757261239216799 \n",
      "Epoch 234:  Loss is 0.017209831761264716 \n",
      "Epoch 235:  Loss is 0.01685949213919484 \n",
      "Epoch 236:  Loss is 0.016521041683189342 \n",
      "Epoch 237:  Loss is 0.01619395659605674 \n",
      "Epoch 238:  Loss is 0.015877739660544052 \n",
      "Epoch 239:  Loss is 0.01557191883708702 \n",
      "Epoch 240:  Loss is 0.015276045925282235 \n",
      "Epoch 241:  Loss is 0.014989695288697931 \n",
      "Epoch 242:  Loss is 0.014712462642080381 \n",
      "Epoch 243:  Loss is 0.014443963899579129 \n",
      "Epoch 244:  Loss is 0.014183834082286564 \n",
      "Epoch 245:  Loss is 0.013931726283146689 \n",
      "Epoch 246:  Loss is 0.013687310687118086 \n",
      "Epoch 247:  Loss is 0.013450273644364717 \n",
      "Epoch 248:  Loss is 0.013220316794183418 \n",
      "Epoch 249:  Loss is 0.012997156237349896 \n",
      "Epoch 250:  Loss is 0.012780521754568619 \n",
      "Epoch 251:  Loss is 0.012570156068737728 \n",
      "Epoch 252:  Loss is 0.012365814148785594 \n",
      "Epoch 253:  Loss is 0.012167262552894265 \n",
      "Epoch 254:  Loss is 0.011974278808994328 \n",
      "Epoch 255:  Loss is 0.01178665083049116 \n",
      "Epoch 256:  Loss is 0.011604176365265508 \n",
      "Epoch 257:  Loss is 0.011426662476074325 \n",
      "Epoch 258:  Loss is 0.011253925050564167 \n",
      "Epoch 259:  Loss is 0.011085788339195877 \n",
      "Epoch 260:  Loss is 0.01092208451946431 \n",
      "Epoch 261:  Loss is 0.010762653284880455 \n",
      "Epoch 262:  Loss is 0.010607341457265815 \n",
      "Epoch 263:  Loss is 0.010456002620987821 \n",
      "Epoch 264:  Loss is 0.01030849677784123 \n",
      "Epoch 265:  Loss is 0.01016469002135467 \n",
      "Epoch 266:  Loss is 0.010024454229371135 \n",
      "Epoch 267:  Loss is 0.00988766677381868 \n",
      "Epoch 268:  Loss is 0.009754210246651294 \n",
      "Epoch 269:  Loss is 0.009623972201000513 \n",
      "Epoch 270:  Loss is 0.009496844906635748 \n",
      "Epoch 271:  Loss is 0.009372725118885768 \n",
      "Epoch 272:  Loss is 0.00925151386022482 \n",
      "Epoch 273:  Loss is 0.00913311621377546 \n",
      "Epoch 274:  Loss is 0.009017441128025884 \n",
      "Epoch 275:  Loss is 0.008904401232102033 \n",
      "Epoch 276:  Loss is 0.008793912660975716 \n",
      "Epoch 277:  Loss is 0.008685894890027468 \n",
      "Epoch 278:  Loss is 0.008580270578418807 \n",
      "Epoch 279:  Loss is 0.008476965420761729 \n",
      "Epoch 280:  Loss is 0.008375908006605313 \n",
      "Epoch 281:  Loss is 0.00827702968728771 \n",
      "Epoch 282:  Loss is 0.008180264449730665 \n",
      "Epoch 283:  Loss is 0.008085548796778563 \n",
      "Epoch 284:  Loss is 0.007992821633708956 \n",
      "Epoch 285:  Loss is 0.007902024160563865 \n",
      "Epoch 286:  Loss is 0.007813099769972905 \n",
      "Epoch 287:  Loss is 0.007725993950158614 \n",
      "Epoch 288:  Loss is 0.007640654192833806 \n",
      "Epoch 289:  Loss is 0.007557029905717684 \n",
      "Epoch 290:  Loss is 0.007475072329414308 \n",
      "Epoch 291:  Loss is 0.007394734458412212 \n",
      "Epoch 292:  Loss is 0.007315970965978271 \n",
      "Epoch 293:  Loss is 0.007238738132732696 \n",
      "Epoch 294:  Loss is 0.007162993778704545 \n",
      "Epoch 295:  Loss is 0.007088697198678979 \n",
      "Epoch 296:  Loss is 0.007015809100658517 \n",
      "Epoch 297:  Loss is 0.00694429154727147 \n",
      "Epoch 298:  Loss is 0.00687410789996963 \n",
      "Epoch 299:  Loss is 0.0068052227658674715 \n",
      "Epoch 300:  Loss is 0.006737601947082821 \n",
      "Epoch 301:  Loss is 0.006671212392447584 \n",
      "Epoch 302:  Loss is 0.006606022151464446 \n",
      "Epoch 303:  Loss is 0.006542000330392799 \n",
      "Epoch 304:  Loss is 0.006479117050353142 \n",
      "Epoch 305:  Loss is 0.006417343407346651 \n",
      "Epoch 306:  Loss is 0.006356651434091188 \n",
      "Epoch 307:  Loss is 0.0062970140635813954 \n",
      "Epoch 308:  Loss is 0.0062384050942854785 \n",
      "Epoch 309:  Loss is 0.006180799156896003 \n",
      "Epoch 310:  Loss is 0.006124171682556776 \n",
      "Epoch 311:  Loss is 0.006068498872492154 \n",
      "Epoch 312:  Loss is 0.006013757668969075 \n",
      "Epoch 313:  Loss is 0.005959925727526206 \n",
      "Epoch 314:  Loss is 0.005906981390407533 \n",
      "Epoch 315:  Loss is 0.005854903661142073 \n",
      "Epoch 316:  Loss is 0.005803672180213621 \n",
      "Epoch 317:  Loss is 0.00575326720176795 \n",
      "Epoch 318:  Loss is 0.00570366957130756 \n",
      "Epoch 319:  Loss is 0.005654860704326807 \n",
      "Epoch 320:  Loss is 0.005606822565842512 \n",
      "Epoch 321:  Loss is 0.005559537650777811 \n",
      "Epoch 322:  Loss is 0.005512988965159 \n",
      "Epoch 323:  Loss is 0.005467160008087305 \n",
      "Epoch 324:  Loss is 0.005422034754449535 \n",
      "Epoch 325:  Loss is 0.005377597638333199 \n",
      "Epoch 326:  Loss is 0.005333833537113914 \n",
      "Epoch 327:  Loss is 0.005290727756183923 \n",
      "Epoch 328:  Loss is 0.0052482660142927715 \n",
      "Epoch 329:  Loss is 0.005206434429472166 \n",
      "Epoch 330:  Loss is 0.005165219505518789 \n",
      "Epoch 331:  Loss is 0.005124608119009883 \n",
      "Epoch 332:  Loss is 0.005084587506827843 \n",
      "Epoch 333:  Loss is 0.005045145254171368 \n",
      "Epoch 334:  Loss is 0.005006269283031237 \n",
      "Epoch 335:  Loss is 0.004967947841110843 \n",
      "Epoch 336:  Loss is 0.004930169491171436 \n",
      "Epoch 337:  Loss is 0.004892923100784107 \n",
      "Epoch 338:  Loss is 0.004856197832470524 \n",
      "Epoch 339:  Loss is 0.0048199831342157825 \n",
      "Epoch 340:  Loss is 0.00478426873033754 \n",
      "Epoch 341:  Loss is 0.0047490446126961135 \n",
      "Epoch 342:  Loss is 0.004714301032230953 \n",
      "Epoch 343:  Loss is 0.0046800284908100785 \n",
      "Epoch 344:  Loss is 0.004646217733378876 \n",
      "Epoch 345:  Loss is 0.004612859740396149 \n",
      "Epoch 346:  Loss is 0.004579945720545129 \n",
      "Epoch 347:  Loss is 0.004547467103708458 \n",
      "Epoch 348:  Loss is 0.004515415534195729 \n",
      "Epoch 349:  Loss is 0.004483782864213948 \n",
      "Epoch 350:  Loss is 0.004452561147570193 \n",
      "Epoch 351:  Loss is 0.0044217426335979035 \n",
      "Epoch 352:  Loss is 0.004391319761296944 \n",
      "Epoch 353:  Loss is 0.004361285153679648 \n",
      "Epoch 354:  Loss is 0.004331631612313886 \n",
      "Epoch 355:  Loss is 0.0043023521120558635 \n",
      "Epoch 356:  Loss is 0.0042734397959648205 \n",
      "Epoch 357:  Loss is 0.004244887970392626 \n",
      "Epoch 358:  Loss is 0.004216690100241388 \n",
      "Epoch 359:  Loss is 0.004188839804382333 \n",
      "Epoch 360:  Loss is 0.004161330851230076 \n",
      "Epoch 361:  Loss is 0.004134157154465919 \n",
      "Epoch 362:  Loss is 0.004107312768904573 \n",
      "Epoch 363:  Loss is 0.004080791886498909 \n",
      "Epoch 364:  Loss is 0.004054588832477271 \n",
      "Epoch 365:  Loss is 0.004028698061608614 \n",
      "Epoch 366:  Loss is 0.0040031141545902645 \n",
      "Epoch 367:  Loss is 0.00397783181455415 \n",
      "Epoch 368:  Loss is 0.003952845863686567 \n",
      "Epoch 369:  Loss is 0.00392815123995777 \n",
      "Epoch 370:  Loss is 0.003903742993956857 \n",
      "Epoch 371:  Loss is 0.003879616285828464 \n",
      "Epoch 372:  Loss is 0.003855766382307145 \n",
      "Epoch 373:  Loss is 0.0038321886538463627 \n",
      "Epoch 374:  Loss is 0.0038088785718379536 \n",
      "Epoch 375:  Loss is 0.003785831705919588 \n",
      "Epoch 376:  Loss is 0.0037630437213664503 \n",
      "Epoch 377:  Loss is 0.003740510376564397 \n",
      "Epoch 378:  Loss is 0.0037182275205615363 \n",
      "Epoch 379:  Loss is 0.0036961910906956733 \n",
      "Epoch 380:  Loss is 0.003674397110294667 \n",
      "Epoch 381:  Loss is 0.0036528416864472996 \n",
      "Epoch 382:  Loss is 0.0036315210078420495 \n",
      "Epoch 383:  Loss is 0.003610431342671692 \n",
      "Epoch 384:  Loss is 0.0035895690366009553 \n",
      "Epoch 385:  Loss is 0.003568930510795568 \n",
      "Epoch 386:  Loss is 0.0035485122600101837 \n",
      "Epoch 387:  Loss is 0.003528310850733345 \n",
      "Epoch 388:  Loss is 0.0035083229193876037 \n",
      "Epoch 389:  Loss is 0.0034885451705827337 \n",
      "Epoch 390:  Loss is 0.0034689743754203334 \n",
      "Epoch 391:  Loss is 0.0034496073698481637 \n",
      "Epoch 392:  Loss is 0.003430441053062384 \n",
      "Epoch 393:  Loss is 0.0034114723859563273 \n",
      "Epoch 394:  Loss is 0.0033926983896138364 \n",
      "Epoch 395:  Loss is 0.003374116143846316 \n",
      "Epoch 396:  Loss is 0.003355722785771389 \n",
      "Epoch 397:  Loss is 0.0033375155084322777 \n",
      "Epoch 398:  Loss is 0.0033194915594564572 \n",
      "Epoch 399:  Loss is 0.0033016482397520207 \n",
      "Epoch 400:  Loss is 0.0032839829022410305 \n",
      "Epoch 401:  Loss is 0.00326649295062814 \n",
      "Epoch 402:  Loss is 0.00324917583820383 \n",
      "Epoch 403:  Loss is 0.0032320290666807295 \n",
      "Epoch 404:  Loss is 0.003215050185062361 \n",
      "Epoch 405:  Loss is 0.0031982367885428193 \n",
      "Epoch 406:  Loss is 0.003181586517436911 \n",
      "Epoch 407:  Loss is 0.0031650970561394407 \n",
      "Epoch 408:  Loss is 0.003148766132112729 \n",
      "Epoch 409:  Loss is 0.003132591514901808 \n",
      "Epoch 410:  Loss is 0.003116571015176061 \n",
      "Epoch 411:  Loss is 0.0031007024837966268 \n",
      "Epoch 412:  Loss is 0.00308498381090885 \n",
      "Epoch 413:  Loss is 0.0030694129250589046 \n",
      "Epoch 414:  Loss is 0.0030539877923338836 \n",
      "Epoch 415:  Loss is 0.003038706415524646 \n",
      "Epoch 416:  Loss is 0.003023566833310681 \n",
      "Epoch 417:  Loss is 0.0030085671194664577 \n",
      "Epoch 418:  Loss is 0.0029937053820884236 \n",
      "Epoch 419:  Loss is 0.002978979762842125 \n",
      "Epoch 420:  Loss is 0.002964388436228984 \n",
      "Epoch 421:  Loss is 0.002949929608871784 \n",
      "Epoch 422:  Loss is 0.0029356015188187552 \n",
      "Epoch 423:  Loss is 0.00292140243486532 \n",
      "Epoch 424:  Loss is 0.0029073306558932903 \n",
      "Epoch 425:  Loss is 0.0028933845102267763 \n",
      "Epoch 426:  Loss is 0.0028795623550043354 \n",
      "Epoch 427:  Loss is 0.0028658625755671784 \n",
      "Epoch 428:  Loss is 0.0028522835848625015 \n",
      "Epoch 429:  Loss is 0.0028388238228618705 \n",
      "Epoch 430:  Loss is 0.002825481755994137 \n",
      "Epoch 431:  Loss is 0.002812255876592355 \n",
      "Epoch 432:  Loss is 0.002799144702354387 \n",
      "Epoch 433:  Loss is 0.0027861467758168957 \n",
      "Epoch 434:  Loss is 0.00277326066384213 \n",
      "Epoch 435:  Loss is 0.002760484957117303 \n",
      "Epoch 436:  Loss is 0.0027478182696662433 \n",
      "Epoch 437:  Loss is 0.0027352592383727385 \n",
      "Epoch 438:  Loss is 0.002722806522515581 \n",
      "Epoch 439:  Loss is 0.002710458803314705 \n",
      "Epoch 440:  Loss is 0.0026982147834882096 \n",
      "Epoch 441:  Loss is 0.0026860731868200687 \n",
      "Epoch 442:  Loss is 0.0026740327577379768 \n",
      "Epoch 443:  Loss is 0.0026620922609014476 \n",
      "Epoch 444:  Loss is 0.0026502504807993854 \n",
      "Epoch 445:  Loss is 0.002638506221357348 \n",
      "Epoch 446:  Loss is 0.002626858305553951 \n",
      "Epoch 447:  Loss is 0.0026153055750461614 \n",
      "Epoch 448:  Loss is 0.0026038468898034865 \n",
      "Epoch 449:  Loss is 0.00259248112775054 \n",
      "Epoch 450:  Loss is 0.0025812071844179036 \n",
      "Epoch 451:  Loss is 0.002570023972601013 \n",
      "Epoch 452:  Loss is 0.0025589304220269534 \n",
      "Epoch 453:  Loss is 0.0025479254790287953 \n",
      "Epoch 454:  Loss is 0.002537008106227407 \n",
      "Epoch 455:  Loss is 0.0025261772822204385 \n",
      "Epoch 456:  Loss is 0.0025154320012784714 \n",
      "Epoch 457:  Loss is 0.0025047712730478316 \n",
      "Epoch 458:  Loss is 0.002494194122260299 \n",
      "Epoch 459:  Loss is 0.0024836995884491334 \n",
      "Epoch 460:  Loss is 0.002473286725671543 \n",
      "Epoch 461:  Loss is 0.002462954602237317 \n",
      "Epoch 462:  Loss is 0.0024527023004434277 \n",
      "Epoch 463:  Loss is 0.002442528916314562 \n",
      "Epoch 464:  Loss is 0.0024324335593492947 \n",
      "Epoch 465:  Loss is 0.002422415352271871 \n",
      "Epoch 466:  Loss is 0.0024124734307894046 \n",
      "Epoch 467:  Loss is 0.002402606943354404 \n",
      "Epoch 468:  Loss is 0.0023928150509323577 \n",
      "Epoch 469:  Loss is 0.002383096926774435 \n",
      "Epoch 470:  Loss is 0.0023734517561950948 \n",
      "Epoch 471:  Loss is 0.0023638787363544204 \n",
      "Epoch 472:  Loss is 0.002354377076045088 \n",
      "Epoch 473:  Loss is 0.0023449459954840365 \n",
      "Epoch 474:  Loss is 0.002335584726108324 \n",
      "Epoch 475:  Loss is 0.002326292510375611 \n",
      "Epoch 476:  Loss is 0.002317068601568585 \n",
      "Epoch 477:  Loss is 0.002307912263603672 \n",
      "Epoch 478:  Loss is 0.0022988227708437056 \n",
      "Epoch 479:  Loss is 0.0022897994079144925 \n",
      "Epoch 480:  Loss is 0.0022808414695252967 \n",
      "Epoch 481:  Loss is 0.0022719482602929345 \n",
      "Epoch 482:  Loss is 0.0022631190945696027 \n",
      "Epoch 483:  Loss is 0.00225435329627419 \n",
      "Epoch 484:  Loss is 0.0022456501987271624 \n",
      "Epoch 485:  Loss is 0.002237009144488725 \n",
      "Epoch 486:  Loss is 0.002228429485200345 \n",
      "Epoch 487:  Loss is 0.002219910581429506 \n",
      "Epoch 488:  Loss is 0.0022114518025176375 \n",
      "Epoch 489:  Loss is 0.002203052526431043 \n",
      "Epoch 490:  Loss is 0.0021947121396149512 \n",
      "Epoch 491:  Loss is 0.0021864300368503904 \n",
      "Epoch 492:  Loss is 0.0021782056211140394 \n",
      "Epoch 493:  Loss is 0.0021700383034407534 \n",
      "Epoch 494:  Loss is 0.0021619275027889837 \n",
      "Epoch 495:  Loss is 0.0021538726459087547 \n",
      "Epoch 496:  Loss is 0.002145873167212263 \n",
      "Epoch 497:  Loss is 0.0021379285086471187 \n",
      "Epoch 498:  Loss is 0.0021300381195719807 \n",
      "Epoch 499:  Loss is 0.002122201456634705 \n",
      " \n",
      "\u001b[32mYour model accuracy is  98.0% on this dataset \u001b[0m\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Dense(2, activation_function=\"reLu\", categories=[-1, 1])\n",
    "\n",
    "b = NetModel(input_shape=(2, ), usage=\"LogisticRegression\")\n",
    "b.add_layer(a)\n",
    "b.compile()\n",
    "\n",
    "X = train_set\n",
    "Y = np.where(train_y==1,\"Fleur ibis\", \"Fleur magnenta\")\n",
    "\n",
    "b.train(X, Y,\"l2\", nepochs= 500, learning_rate=0.01)\n",
    "b.display_losses()\n",
    "x = test_set\n",
    "y = np.where(test_y==1,\"Fleur ibis\", \"Fleur magnenta\")\n",
    "z=b.predict_sample(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fleur magnenta', 'Fleur ibis', 'Fleur magnenta', 'Fleur ibis', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta', 'Fleur ibis', 'Fleur ibis', 'Fleur ibis', 'Fleur magnenta', 'Fleur ibis', 'Fleur ibis', 'Fleur magnenta', 'Fleur magnenta', 'Fleur ibis', 'Fleur ibis', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta', 'Fleur ibis', 'Fleur magnenta', 'Fleur ibis', 'Fleur ibis', 'Fleur magnenta', 'Fleur magnenta', 'Fleur ibis', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta', 'Fleur ibis', 'Fleur ibis', 'Fleur ibis', 'Fleur magnenta', 'Fleur ibis', 'Fleur ibis', 'Fleur ibis', 'Fleur magnenta', 'Fleur ibis', 'Fleur ibis', 'Fleur magnenta', 'Fleur magnenta', 'Fleur ibis', 'Fleur magnenta', 'Fleur magnenta', 'Fleur magnenta']\n",
      "\n",
      " ['Fleur magnenta' 'Fleur ibis' 'Fleur magnenta' 'Fleur ibis'\n",
      " 'Fleur magnenta' 'Fleur magnenta' 'Fleur ibis' 'Fleur magnenta'\n",
      " 'Fleur ibis' 'Fleur ibis' 'Fleur ibis' 'Fleur magnenta' 'Fleur ibis'\n",
      " 'Fleur ibis' 'Fleur magnenta' 'Fleur magnenta' 'Fleur ibis' 'Fleur ibis'\n",
      " 'Fleur magnenta' 'Fleur magnenta' 'Fleur magnenta' 'Fleur magnenta'\n",
      " 'Fleur magnenta' 'Fleur magnenta' 'Fleur ibis' 'Fleur magnenta'\n",
      " 'Fleur ibis' 'Fleur ibis' 'Fleur magnenta' 'Fleur magnenta' 'Fleur ibis'\n",
      " 'Fleur magnenta' 'Fleur magnenta' 'Fleur magnenta' 'Fleur ibis'\n",
      " 'Fleur ibis' 'Fleur ibis' 'Fleur magnenta' 'Fleur ibis' 'Fleur ibis'\n",
      " 'Fleur ibis' 'Fleur magnenta' 'Fleur ibis' 'Fleur ibis' 'Fleur magnenta'\n",
      " 'Fleur magnenta' 'Fleur ibis' 'Fleur magnenta' 'Fleur magnenta'\n",
      " 'Fleur magnenta']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (z)\n",
    "print(\"\\n\", y)\n",
    "sum(z[i] == y[i] for i in range(len(y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
